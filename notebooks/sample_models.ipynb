{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "source": [
    "import getopt\n",
    "import sys\n",
    "\n",
    "if '-i' in sys.argv:\n",
    "    myopts, args = getopt.getopt(sys.argv[1:],\"i:c:\")\n",
    "\n",
    "    for o, a in myopts:\n",
    "        if o == '-i':\n",
    "            i=int(a)\n",
    "        elif o == '-c':\n",
    "            n_cores=int(a)\n",
    "        else:\n",
    "            print(\"Usage: %s -i iteration -n_cores n_cores\" % sys.argv[0])\n",
    "\n",
    "    # Display input and output file name passed as the args\n",
    "    print (\"Running: %d with n_cores: %d\" % (i, n_cores) )\n",
    "else:\n",
    "    i = 0\n",
    "    n_cores=11\n",
    "    print('Running locally')\n",
    "\n",
    "# set theano flags\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"compiledir=./theano/%s/\" %(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for smooth switch point:\n",
    "# https://gist.github.com/junpenglao/f7098c8e0d6eadc61b3e1bc8525dd90d\n",
    "import theano.tensor as tt\n",
    "from pymc3.distributions.transforms import ElemwiseTransform, Transform\n",
    "\n",
    "class Ordered(ElemwiseTransform):\n",
    "    name = \"ordered\"\n",
    "\n",
    "    def backward(self, y):\n",
    "        out = tt.zeros(y.shape)\n",
    "        out = tt.inc_subtensor(out[0], y[0])\n",
    "        out = tt.inc_subtensor(out[1:], tt.exp(y[1:]))\n",
    "        return tt.cumsum(out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = tt.zeros(x.shape)\n",
    "        out = tt.inc_subtensor(out[0], x[0])\n",
    "        out = tt.inc_subtensor(out[1:], tt.log(x[1:] - x[:-1]))\n",
    "        return out\n",
    "\n",
    "    def forward_val(self, x, point=None):\n",
    "        x, = draw_values([x], point=point)\n",
    "        return self.forward(x)\n",
    "\n",
    "    def jacobian_det(self, y):\n",
    "        return tt.sum(y[1:])\n",
    "\n",
    "ordered = Ordered()\n",
    "\n",
    "\n",
    "class Composed(Transform):\n",
    "    def __init__(self, transform1, transform2):\n",
    "        self._transform1 = transform1\n",
    "        self._transform2 = transform2\n",
    "        self.name = '_'.join([transform1.name, transform2.name])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._transform2.forward(self._transform1.forward(x))\n",
    "\n",
    "    def forward_val(self, x, point=None):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, y):\n",
    "        return self._transform1.backward(self._transform2.backward(y))\n",
    "\n",
    "    def jacobian_det(self, y):\n",
    "        y2 = self._transform2.backward(y)\n",
    "        det1 = self._transform1.jacobian_det(y2)\n",
    "        det2 = self._transform2.jacobian_det(y)\n",
    "        return det1 + det2\n",
    "    \n",
    "def logistic(L, x0, k=50, t_=np.linspace(0., 1., 1000)):\n",
    "    x0 = x0*(t_.max()-t_.min()) + t_.min()  # scale x0 to t_\n",
    "    return L/(1+tt.exp(-k*(t_-x0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_model(df, model_n, distribution, dep_var='rate', n_cores=15):\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        intercept = pm.Normal('intercept', mu=1, sd=3)\n",
    "\n",
    "        if model_n == 1:\n",
    "            # no change\n",
    "            ev = np.exp(intercept)\n",
    "\n",
    "        elif model_n == 2:\n",
    "            # gradient along pc axis 1\n",
    "            beta_pca_1 = pm.Normal('beta_pca_1', mu=0, sd=3)\n",
    "            ev = np.exp(intercept + beta_pca_1*df['pc1_mm'].values)\n",
    "\n",
    "        elif model_n == 3:\n",
    "            # 3 sectors along pc axis 1\n",
    "            delta_center_1 = pm.Normal('delta_center_1', mu=0, sd=3)\n",
    "            delta_center_3 = pm.Normal('delta_center_3', mu=0, sd=3)\n",
    "\n",
    "            ev = np.exp(intercept + \\\n",
    "                        delta_center_1*((df['pc1_mm_perc'].values<0.333).astype(int)) + \\\n",
    "                        delta_center_3*((df['pc1_mm_perc'].values>0.667).astype(int)))\n",
    "\n",
    "        elif model_n == 4:\n",
    "            # gradient along pc axis 1+2+3\n",
    "            beta_pca_1 = pm.Normal('beta_pca_1', mu=0, sd=3)\n",
    "            beta_pca_2 = pm.Normal('beta_pca_2', mu=0, sd=3)\n",
    "            beta_slice = pm.Normal('beta_slice', mu=0, sd=3)\n",
    "\n",
    "            # np.cdot() # betas met coordinaten  ->  coordinaat op deze nieuwe as\n",
    "            # # cut_off op die as [0,1]\n",
    "            # # per voxel bepalen welke sector\n",
    "\n",
    "            ev = np.exp(intercept + beta_pca_1*df['pc1_mm'].values + \\\n",
    "                        beta_pca_2*df['pc2_mm'].values + \\\n",
    "                        beta_slice*df['slice_sector'].values)\n",
    "\n",
    "        elif model_n == 5:\n",
    "            # cut-offs estimated, smoothness of cut-off fixed to 50\n",
    "            nbreak = 3\n",
    "            lambdad = pm.Normal('lambdad', 0, sd=1, shape=3-1)\n",
    "            trafo = Composed(pm.distributions.transforms.LogOdds(), Ordered())\n",
    "            b = pm.Beta('b', 4., 4., shape=nbreak-1, transform=trafo,\n",
    "                        testval=[0.33, 0.67])\n",
    "            ev = np.exp(intercept + intercept*logistic(lambdad[0], b[0], k=-50, t_=df['pc1_mm_perc'].values) +\n",
    "                                    intercept*logistic(lambdad[1], b[1], k=50, t_=df['pc1_mm_perc'].values))\n",
    "        elif model_n == 6:\n",
    "            # cut-offs estimated, smoothness of cut-off estimated\n",
    "            nbreak = 3\n",
    "            lambdad = pm.Normal('lambdad', 0, sd=1, shape=3-1)\n",
    "            k = pm.HalfStudentT('k', 1, 2, shape=2)\n",
    "            trafo = Composed(pm.distributions.transforms.LogOdds(), Ordered())\n",
    "            b = pm.Beta('b', 4., 4., shape=nbreak-1, transform=trafo,\n",
    "                        testval=[0.33, 0.67])\n",
    "            ev = np.exp(intercept + intercept*logistic(lambdad[0], b[0], k=-np.exp(k[0]), t_=df['pc1_mm_perc'].values) +\n",
    "                                    intercept*logistic(lambdad[1], b[1], k=np.exp(k[1]), t_=df['pc1_mm_perc'].values))\n",
    "            \n",
    "        elif model_n == 7:\n",
    "            # cut-offs estimated, smoothness of cut-off fixed to 50, cut-offs determined in 3D\n",
    "            beta_pca_1 = pm.HalfNormal('beta_pca_1', sd=3)\n",
    "            beta_pca_2 = pm.HalfNormal('beta_pca_2', sd=3)\n",
    "            beta_slice = pm.HalfNormal('beta_slice', sd=3)\n",
    "            trafo = Composed(pm.distributions.transforms.LogOdds(), Ordered())\n",
    "            b = pm.Beta('b', 4., 4., shape=2, transform=trafo, testval=[0.33, 0.67])\n",
    "            lambdad = pm.Normal('lambdad', 0, sd=1, shape=3-1)\n",
    "            \n",
    "            # use principal components in percentage for 0-1 scale\n",
    "            O_vec = beta_pca_1*df['pc1_mm_perc'].values + \\\n",
    "                    beta_pca_2*df['pc2_mm_perc'].values + \\\n",
    "                    beta_slice*df['slice_sector_perc'].values\n",
    "        \n",
    "            ev = np.exp(intercept + intercept*logistic(lambdad[0], b[0], k=-50, t_=O_vec) +\n",
    "                                    intercept*logistic(lambdad[1], b[1], k=50, t_=O_vec))\n",
    "        \n",
    "        elif model_n == 8:\n",
    "            # cut-offs estimated, smoothness of cut-off fixed to 50, cut-offs determined in 3D\n",
    "            beta_pca_1 = pm.HalfNormal('beta_pca_1', sd=3)\n",
    "            beta_pca_2 = pm.HalfNormal('beta_pca_2', sd=3)\n",
    "            beta_slice = pm.HalfNormal('beta_slice', sd=3)\n",
    "            trafo = Composed(pm.distributions.transforms.LogOdds(), Ordered())\n",
    "            b = pm.Beta('b', 4., 4., shape=2, transform=trafo, testval=[0.33, 0.67])\n",
    "            lambdad = pm.Normal('lambdad', 0, sd=1, shape=3-1)\n",
    "            \n",
    "            # use principal components in mm? may sample better?\n",
    "            O_vec = beta_pca_1*df['pc1_mm'].values + \\\n",
    "                    beta_pca_2*df['pc2_mm'].values + \\\n",
    "                    beta_slice*df['slice_mm'].values\n",
    "        \n",
    "            ev = np.exp(intercept + intercept*logistic(lambdad[0], b[0], k=-50, t_=O_vec) +\n",
    "                                    intercept*logistic(lambdad[1], b[1], k=50, t_=O_vec))\n",
    "\n",
    "        # define likelihood\n",
    "        if distribution == 'poisson':\n",
    "            likelihood = pm.Poisson('y', mu=ev, observed=df[dep_var])\n",
    "        else:\n",
    "            alpha = pm.HalfCauchy('alpha', beta=2)\n",
    "            likelihood = pm.NegativeBinomial('y', mu=ev, alpha=alpha, observed=df[dep_var])\n",
    "\n",
    "        model.name = str(model_n) + '_' + distribution\n",
    "        traces = pm.sample(cores=n_cores)\n",
    "        \n",
    "        return model, traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_pickle('./data.pkl')\n",
    "subjects = df.subject_id.unique()\n",
    "stains = df.stain.unique()\n",
    "models = [8,7,6,5,4,3,2,1]\n",
    "distributions = ['poisson', 'negativebinomial']\n",
    "\n",
    "output_dir = './models'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "all_combs = list(itertools.product(subjects, stains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject, stain = all_combs[i]\n",
    "df_to_run = df.loc[(df.subject_id==subject) & (df.stain==stain),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_n in models:\n",
    "    for distribution in distributions:\n",
    "        print('Subject {}, stain {}, model {}, distribution {}'.format(subject, stain, model_n, distribution))\n",
    "        trace_fn = os.path.join(output_dir, 'sub-{}_stain-{}_model-{}_distribution-{}_type-traces.pkl').format(subject, stain, model_n, distribution)\n",
    "        model_fn = os.path.join(output_dir, 'sub-{}_stain-{}_model-{}_distribution-{}_type-model.pkl').format(subject, stain, model_n, distribution)\n",
    "        if os.path.exists(trace_fn):\n",
    "            continue\n",
    "        \n",
    "        model, traces = sample_single_model(df_to_run, model_n, distribution, n_cores=n_cores)\n",
    "        with open(model_fn, 'wb') as f:\n",
    "            pkl.dump(model, f)\n",
    "        with open(trace_fn, 'wb') as f:\n",
    "            pkl.dump(traces, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def np_logistic(L, x0, k=50, t_=np.linspace(0., 1., 1000)):\n",
    "#     x0 = x0*(t_.max()-t_.min()) + t_.min()  # scale x0 to t_\n",
    "#     return L/(1+np.exp(-k*(t_-x0)))\n",
    "\n",
    "# def ppc_stn(df, trace, distribution='poisson', model_n=1, n_samples=500):\n",
    "    \n",
    "#     trace_df = pm.trace_to_dataframe(trace)\n",
    "#     out_array = np.empty((df.shape[0], n_samples))\n",
    "\n",
    "#     for i in np.arange(n_samples):\n",
    "#         if i % 10 == 0:\n",
    "#             print('.', end='')\n",
    "#         random_row = np.random.randint(low=0, high=trace_df.shape[0])\n",
    "        \n",
    "#         if model_n == 1:\n",
    "#             ev = np.exp(trace_df.iloc[random_row]['intercept'])\n",
    "#         elif model_n == 2:\n",
    "#             ev = np.exp(trace_df.iloc[random_row]['intercept'] + \\\n",
    "#                         trace_df.iloc[random_row]['beta_pca_1']*df['pc1_sector_coordinate'].values)\n",
    "#         elif model_n == 3:\n",
    "#             pass\n",
    "#         elif model_n == 4:\n",
    "#             ev = np.exp(trace_df.iloc[random_row]['intercept'] + \\\n",
    "#                         trace_df.iloc[random_row]['beta_pca_1']*df['pc1_sector_coordinate'].values + \\\n",
    "#                         trace_df.iloc[random_row]['beta_pca_2']*df['pc2_sector_coordinate'].values + \\\n",
    "#                         trace_df.iloc[random_row]['beta_slice']*df['slice_sector_coordinate'].values)\n",
    "            \n",
    "#         elif model_n == 5:\n",
    "#             this_int = trace_df.iloc[random_row]['intercept']\n",
    "#             this_lambda_0 = trace_df.iloc[random_row]['lambdad__0']\n",
    "#             this_lambda_1 = trace_df.iloc[random_row]['lambdad__1']\n",
    "#             this_b_0 = trace_df.iloc[random_row]['b__0']\n",
    "#             this_b_1 = trace_df.iloc[random_row]['b__1']\n",
    "#             ev = np.exp(this_int + \\\n",
    "#                         this_int*np_logistic(this_lambda_0, this_b_0, k=-50, t_=df['pc1_mm_perc'].values) +\\\n",
    "#                         this_int*np_logistic(this_lambda_1, this_b_1, k=50, t_=df['pc1_mm_perc'].values))\n",
    "            \n",
    "#         elif model_n == 6:\n",
    "#             this_int = trace_df.iloc[random_row]['intercept']\n",
    "#             this_lambda_0 = trace_df.iloc[random_row]['lambdad__0']\n",
    "#             this_lambda_1 = trace_df.iloc[random_row]['lambdad__1']\n",
    "#             this_b_0 = trace_df.iloc[random_row]['b__0']\n",
    "#             this_b_1 = trace_df.iloc[random_row]['b__1']\n",
    "#             this_k_0 = trace_df.iloc[random_row]['k__0']\n",
    "#             this_k_1 = trace_df.iloc[random_row]['k__1']\n",
    "#             ev = np.exp(this_int + \\\n",
    "#                         this_int*np_logistic(this_lambda_0, this_b_0, k=-np.exp(this_k_0), t_=df['pc1_mm_perc'].values) +\\\n",
    "#                         this_int*np_logistic(this_lambda_1, this_b_1, k=np.exp(this_k_1), t_=df['pc1_mm_perc'].values))\n",
    "                        \n",
    "#         elif model_n == 7:\n",
    "#             this_int = trace_df.iloc[random_row]['intercept']\n",
    "#             this_lambda_0 = trace_df.iloc[random_row]['lambdad__0']\n",
    "#             this_lambda_1 = trace_df.iloc[random_row]['lambdad__1']\n",
    "#             this_b_0 = trace_df.iloc[random_row]['b__0']\n",
    "#             this_b_1 = trace_df.iloc[random_row]['b__1']\n",
    "#             this_beta_1 = trace_df.iloc[random_row]['beta_pca_1']\n",
    "#             this_beta_2 = trace_df.iloc[random_row]['beta_pca_2']\n",
    "#             this_beta_slice = trace_df.iloc[random_row]['beta_slice']\n",
    "            \n",
    "#             O_vec = this_beta_1*df['pc1_mm_perc'].values + \\\n",
    "#                     this_beta_2*df['pc2_mm_perc'].values + \\\n",
    "#                     this_beta_slice*df['slice_sector_perc'].values\n",
    "#             # set O to scale 0-1, use this for cut-off\n",
    "# #            O_vec = (O_vec-O_vec.min()/(O_vec.max()-O_vec.min()))\n",
    "\n",
    "#             ev = np.exp(this_int + \\\n",
    "#                         this_int*np_logistic(this_lambda_0, this_b_0, k=-50, t_=O_vec) +\\\n",
    "#                         this_int*np_logistic(this_lambda_1, this_b_1, k=50, t_=O_vec))\n",
    "                        \n",
    "#         if distribution == 'poisson':\n",
    "#             out_array[:,i] = ev\n",
    "#         else:\n",
    "#             out_array[:,i] = ev  # do something with variance as well?\n",
    "#     return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('colormap', ['blue', 'lightgray', 'red'])\n",
    "def plot_ellipse_values(values, ellipse_pars=None, size=(1000, 1000), vmin=None, vmax=None, cmap=plt.cm.coolwarm, ax=None, **kwargs):\n",
    "\n",
    "    ''' values is a n-by-m array'''\n",
    "\n",
    "    values[np.isnan(values)] = 0\n",
    "    if ellipse_pars is None:\n",
    "        a = 350\n",
    "        b = 150\n",
    "        x = 500\n",
    "        y = 500\n",
    "\n",
    "        theta = 45. / 180 * np.pi\n",
    "\n",
    "    else:\n",
    "        a, b, x, y, theta = ellipse_pars\n",
    "\n",
    "    A = a**2 * (np.sin(theta))**2 + b**2 * (np.cos(theta))**2\n",
    "    B = 2 * (b**2 - a**2) * np.sin(theta) * np.cos(theta)\n",
    "    C = a**2 * np.cos(theta)**2 + b**2 * np.sin(theta)**2\n",
    "    D = -2 * A * x - B* y\n",
    "    E = -B * x - 2 * C * y\n",
    "    F = A* x**2 + B*x*y + C*y**2 - a**2*b**2\n",
    "\n",
    "    X,Y = np.meshgrid(np.arange(size[0]), np.arange(size[1]))\n",
    "\n",
    "    in_ellipse = A*X**2 + B*X*Y +C*Y**2 + D*X + E*Y +F < 0\n",
    "\n",
    "    pc1 = np.array([[np.cos(theta)], [np.sin(theta)]])\n",
    "    pc2 = np.array([[np.cos(theta - np.pi/2.)], [np.sin(theta - np.pi/2.)]])\n",
    "\n",
    "    pc1_distance = pc1.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "    pc2_distance = pc2.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "\n",
    "    pc1_quantile = np.floor((pc1_distance / a + 1 ) / 2. * values.shape[0])\n",
    "    pc2_quantile = np.floor((pc2_distance / b + 1 ) / 2. * values.shape[1])\n",
    "\n",
    "    im = np.zeros_like(X, dtype=float)\n",
    "\n",
    "    for pc1_q in np.arange(values.shape[0]):\n",
    "        for pc2_q in np.arange(values.shape[1]):\n",
    "            im[in_ellipse * (pc1_quantile == pc1_q) & (pc2_quantile == pc2_q)] = values[pc1_q, pc2_q]\n",
    "\n",
    "    im = np.ma.masked_array(im, ~in_ellipse)\n",
    "#     cmap.set_bad('grey')\n",
    "    if ax is None:\n",
    "        cax = plt.imshow(im, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\n",
    "    else:\n",
    "        ax.imshow(im, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\n",
    "        cax = ax\n",
    "#    sns.despine()\n",
    "\n",
    "    return cax\n",
    "\n",
    "def visualize_stn_model(df, dependent_var='y', ax=None, vmin=None, vmax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(1, 1)\n",
    "    vmd_labels = df['pc1_sector_name'].unique()\n",
    "    mml_labels = df['pc2_sector_name'].unique()\n",
    "    \n",
    "    unstacked = df.groupby(['pc1_sector_name', 'pc2_sector_name'])[dependent_var].mean().unstack(1).ix[vmd_labels, mml_labels]\n",
    "\n",
    "    if vmin is None:\n",
    "        vmin = np.nanpercentile(unstacked.values, 5)\n",
    "    if vmax is None:\n",
    "        vmax = np.nanpercentile(unstacked.values, 95)\n",
    "    plot_ellipse_values(unstacked.values, ax=ax, vmin=vmin, vmax=vmax, **kwargs)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def plot_intensity_across_axis(df, dependent_var='y', x_axis='pc1_mm', ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(1, 1)\n",
    "        \n",
    "    data_per_coordinate = df.groupby([x_axis])[dependent_var].mean().reset_index()\n",
    "    ax.plot(data_per_coordinate[x_axis], data_per_coordinate[dependent_var], **kwargs)\n",
    "    ax.set_xlabel(x_axis)\n",
    "    ax.set_ylabel('Rate')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id = 13095 #13095#14037\n",
    "stain = 'CALR'\n",
    "model_n = 7\n",
    "# data_df_idx = (df.subject_id==subj_id) & (df.stain==stain)\n",
    "# model_df_idx = (subj_id, 'poisson', stain, str(model_n))\n",
    "\n",
    "ppc = ppc_stn(df=df_to_run, trace=traces, model_n=model_n, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_mean = ppc.mean(axis=1)\n",
    "df_tmp = df_to_run.copy()\n",
    "df_tmp['y_predicted'] = ppc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax_stn_data = plt.subplot(gs[0,0])\n",
    "ax_stn_model = plt.subplot(gs[0,1])\n",
    "ax_graph = plt.subplot(gs[1,:])\n",
    "vmin = np.nanpercentile(df_tmp['rate'], 5)\n",
    "vmax = np.nanpercentile(df_tmp['rate'], 95)\n",
    "\n",
    "visualize_stn_model(df_tmp, dependent_var='rate', ax=ax_stn_data, vmin=vmin, vmax=vmax)\n",
    "visualize_stn_model(df_tmp, dependent_var='y_predicted', ax=ax_stn_model, vmin=vmin, vmax=vmax)\n",
    "plot_intensity_across_axis(df_tmp, dependent_var='rate', ax=ax_graph, x_axis='pc1_sector_number', label='Data')\n",
    "plot_intensity_across_axis(df_tmp, dependent_var='y_predicted', ax=ax_graph, x_axis='pc1_sector_number', label='Model')\n",
    "\n",
    "ax_stn_data.set_title('Data')\n",
    "ax_stn_model.set_title('Model')\n",
    "ax_graph.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(6, 4)\n",
    "\n",
    "vmin = np.nanpercentile(df_tmp['rate'], 5)\n",
    "vmax = np.nanpercentile(df_tmp['rate'], 95)\n",
    "for row_n, slice_id in enumerate([1,5,8,10,15]):\n",
    "    df_this_slice = df_tmp.loc[df_tmp.slice_sector==df_tmp.slice_sector.unique()[slice_id]]\n",
    "    ax_stn_data = plt.subplot(gs[row_n,0])\n",
    "    ax_stn_model = plt.subplot(gs[row_n,1])\n",
    "    ax_graph_pc1 = plt.subplot(gs[row_n,2])\n",
    "    ax_graph_pc2 = plt.subplot(gs[row_n,3])\n",
    "\n",
    "    visualize_stn_model(df_this_slice, dependent_var='rate', ax=ax_stn_data, vmin=vmin, vmax=vmax)\n",
    "    visualize_stn_model(df_this_slice, dependent_var='y_predicted', ax=ax_stn_model, vmin=vmin, vmax=vmax)\n",
    "    plot_intensity_across_axis(df_this_slice, dependent_var='rate', ax=ax_graph_pc1, label='Data')\n",
    "    plot_intensity_across_axis(df_this_slice, dependent_var='y_predicted', ax=ax_graph_pc1, label='Model')\n",
    "    plot_intensity_across_axis(df_this_slice, x_axis='pc2_mm', dependent_var='rate', ax=ax_graph_pc2, label='Data')\n",
    "    plot_intensity_across_axis(df_this_slice, x_axis='pc2_mm', dependent_var='y_predicted', ax=ax_graph_pc2, label='Model')\n",
    "\n",
    "    ax_stn_data.set_title('Data')\n",
    "    ax_stn_model.set_title('Model')\n",
    "    ax_graph_pc1.legend()\n",
    "    ax_graph_pc1.set_ylim(vmin, vmax)\n",
    "    ax_graph_pc2.legend()\n",
    "    ax_graph_pc2.set_ylim(vmin, vmax)\n",
    "\n",
    "# some overall plots (across pc1, pc2, slice)\n",
    "plot_intensity_across_axis(df_tmp, x_axis='pc1_sector_number', dependent_var='rate', ax=plt.subplot(gs[-1,0]), label='Data')\n",
    "plot_intensity_across_axis(df_tmp, x_axis='pc1_sector_number', dependent_var='y_predicted', ax=plt.subplot(gs[-1,0]), label='Model')\n",
    "plot_intensity_across_axis(df_tmp, x_axis='pc2_sector_number', dependent_var='rate', ax=plt.subplot(gs[-1,1]), label='Data')\n",
    "plot_intensity_across_axis(df_tmp, x_axis='pc2_sector_number', dependent_var='y_predicted', ax=plt.subplot(gs[-1,1]), label='Model')\n",
    "plot_intensity_across_axis(df_tmp, x_axis='slice_sector', dependent_var='rate', ax=plt.subplot(gs[-1,2]), label='Data')\n",
    "plot_intensity_across_axis(df_tmp, x_axis='slice_sector', dependent_var='y_predicted', ax=plt.subplot(gs[-1,2]), label='Model')\n",
    "\n",
    "plt.gcf().set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
