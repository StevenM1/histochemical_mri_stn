{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project name: \n",
    "    The Functional Neuroanatomy of the Human Subthalamic Nucleus\n",
    "    \n",
    "    Initial code by Gilles de Hollander; \n",
    "    Edited by Steven Miletic and Max Keuken\n",
    "\n",
    "# Goal of the project: \n",
    "    To investigate the internal organisation of the human subthalamic nucleus using histology. The non-demented control tissue has been originally analyzed by Gilles de Hollander. \n",
    "\n",
    "# Layout of the Notebook\n",
    "### 1) Combine and store the data:\n",
    "    1) import histo data into a HDF5 file that contains the histo data and the STN masks in the folder:\n",
    "       /home/mkeuken1/data/post_mortem/new_data_format/\n",
    "\n",
    "### 2) Plot the data:\n",
    "    2) load in the HDF5 data files using the base.py script. The base.py script loads in the data, sets the resolution but also smooths the data with a number of smoothing kernels (0.15, 0.3, 0.6, 1.2, and 2.4 mm fwhm). The reason why we have such a large number of smoothing kernels is because we ran a simulation to generate a hypothesis figure. Here we noted that if the smoothing kernel was too small the histograms were very noisy, if the smoothing kernel was very large you cannot find small transition zones. So there needed to be a bit of a balance. We decided to run the entire analysis for the 5 different smoothing kernels. The final choice of fwhm to report in the main manuscript was based on consistency accross tissue blocks, the other kernels were placed in the supplements.  \n",
    "\n",
    "### 3) Statistical analysis of the 27 PCA intensity sectors\n",
    "    3a) Creating the 27 PCA sectors where for each stain, across the subjects we will test whether they differ from 0\n",
    "    3b) Doing the actual statistical testing: t-tests which are FDR corrected for multiple comparisons.\n",
    "\n",
    "### 4) Mixture models based \n",
    "    4a1) Creating the feature vectors (intensity and gradient) for the different stains\n",
    "    4a2) Fit the mixture models to the feature vectors (intensity and gradient)\n",
    "    4b) Sanity check for the gradient vectors: create same plots for gradient as is done in step 2)\n",
    "    4c) How many clusters fit the data best?\n",
    "    4c1) Model selection based on AIC and BIC\n",
    "    4c2) Model selection based on cross validation within and between\n",
    "    \n",
    "\n",
    "### Differences compared to the initial analysis as shown in the thesis by Gilles de Hollander\n",
    "- The input data contains a few more slices (were initially not included due to naming errors)\n",
    "- Different amount of samples (now 15% of the data)\n",
    "Mixture analysis has been changed substantially:\n",
    "\n",
    "- different number of train / test partitions and number of samples. See \"def make_feature_vector\" in cluster.py for the updated code.\n",
    "- treat the BIC and AIC seperately, see section 4c1).\n",
    "\n",
    "THIS MEANS THAT YOU CANNOT USE THE ORIGINAL CODE ON GITHUB AS THERE ARE DIFFERENT FUNCTIONS USED.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Combine and store the data\n",
    "#### Importing the histological data as well as the masks of the STN and save them into a HDF5 file.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# What is the exact dataset that we are working with?\n",
    "############\n",
    "\n",
    "# The stain data of the following tissue blocks: 13095, 14037, 14051, 14069, 15033, 15035, 15055 \n",
    "# \n",
    "# The specific data files are the processed files that will also be shared via DANS/Figshare. \n",
    "# The DANS/Figshare has the following folder structure:\n",
    "#   Subject ID/\n",
    "#              stain/\n",
    "#                    unique stain/\n",
    "#                                 orig/ (not relevant for this project, the multipage tiff as from the microscope)\n",
    "#                                 proc/ (these are the files we will use for this project)\n",
    "#              blockface/               (not relevant for this project)\n",
    "#              MRI/                     (not relevant for this project)\n",
    "#\n",
    "# The stain data in the proc/ folder is aligned to the Blockface space\n",
    "#\n",
    "# All stain to blockface registration steps were visually inspected by Anneke Alkemade. If the registration failed, \n",
    "#   this stain and slice was excluded. See \"exclusion_list.txt\" for an overview. \n",
    "# \n",
    "# For this project the processed .png files (as indicated in the proc.DANS/Figshare folder) were renamed and\n",
    "#   copied to the following folder:\n",
    "#      data/STN_Histo/stacked_slides/\n",
    "#\n",
    "# How were the files renamed?\n",
    "#    13095_vglut1_proc_1800_7561_2_blockface.png -> 13095_vglut1_1800_7561.png\n",
    "#\n",
    "#  and moved to their respective subjectID folder:\n",
    "#    data/STN_Histo/stacked_slides/subjectID/\n",
    "#\n",
    "############\n",
    "# Start code\n",
    "############\n",
    "\n",
    "# Importing a number of different tools\n",
    "import re\n",
    "import pandas\n",
    "import glob\n",
    "import h5py\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "import natsort\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Find the stains.png images per tissue blocks that have been registered to the blockface images\n",
    "fns = glob.glob('/home/mkeuken1/data/post_mortem/stacked_slides/*/*')\n",
    "reg = re.compile('.*/(?P<subject_id>[0-9]{5})_png/(?P<stain>[A-Za-z0-9]+)_(?P<slice>[0-9]+)_[0-9]+_(?P<id>[0-9]+)\\.png')\n",
    "\n",
    "df = pandas.DataFrame([reg.match(fn).groupdict() for fn in fns if reg.match(fn)])\n",
    "df['subject_id'] = df['subject_id'].astype(int)\n",
    "df['slice'] = df['slice'].astype(int)\n",
    "df['fn'] = [fn for fn in fns if reg.match(fn)]\n",
    "df['id'] = df['id'].astype(int)\n",
    "# There were a number of stains where there were 2 images. The first image was before the tissue block\n",
    "#  was moved forwards again during the cutting. The second image was once the cutting continued. We chose\n",
    "#  to only keep the second image:\n",
    "df = df.drop_duplicates(['subject_id', 'slice', 'stain'], keep='last')\n",
    "\n",
    "# The naming conventions of the stains was lower case so rename to match to uppercase\n",
    "def correct_stain(stain):\n",
    "    if stain == 'calr':\n",
    "        return 'CALR'\n",
    "    \n",
    "    if stain == 'fer':\n",
    "        return 'FER'\n",
    "\n",
    "    if stain == 'gabra3':\n",
    "        return 'GABRA3'\n",
    "    \n",
    "    if stain == 'gad6567':\n",
    "        return 'GAD6567'\n",
    "    \n",
    "    if stain == 'mbp':\n",
    "        return 'MBP'\n",
    "    \n",
    "    if stain == 'parv':\n",
    "        return 'PARV'    \n",
    "        \n",
    "    if stain == 'sert':\n",
    "        return 'SERT' \n",
    "    \n",
    "    if stain == 'smi32':\n",
    "        return 'SMI32' \n",
    "    \n",
    "    if stain == 'syn':\n",
    "        return 'SYN'   \n",
    "    \n",
    "    if stain == 'th':\n",
    "        return 'TH' \n",
    "    \n",
    "    if stain == 'transf':\n",
    "        return 'TRANSF' \n",
    "    \n",
    "    if stain == 'vglut1':\n",
    "        return 'VGLUT1'\n",
    "    \n",
    "    return stain\n",
    "\n",
    "df['stain'] = df.stain.map(correct_stain).astype(str)\n",
    "\n",
    "# Make a data structure that will be used for combining the histo data\n",
    "df.to_pickle('/home/mkeuken1/data/post_mortem/data.pandas')\n",
    "\n",
    "# Find the masks of the STN that were based of two raters who parcellated the STN using the PARV and SMI32 stains.\n",
    "reg3 = re.compile('/home/mkeuken1/data/post_mortem/histo_masks/(?P<subject_id>[0-9]{5})_RegMasks_(?P<rater>[A-Z]+)/(?P<stain>[A-Z0-9a-z_]+)_(?P<slice>[0-9]+)_([0-9]+)_(?P<id>[0-9]+)\\.png')\n",
    "\n",
    "fns = glob.glob('/home/mkeuken1/data/post_mortem/histo_masks/*_RegMasks_*/*_*_*_*.png')\n",
    "\n",
    "masks = pandas.DataFrame([reg3.match(fn).groupdict() for fn in fns])\n",
    "masks['fn'] = fns\n",
    "masks['subject_id'] = masks['subject_id'].astype(int)\n",
    "masks['slice'] = masks['slice'].astype(int)\n",
    "\n",
    "masks.set_index(['subject_id', 'slice', 'stain', 'rater'], inplace=True)\n",
    "masks.sort_index(inplace=True)\n",
    "\n",
    "masks.to_pickle('/home/mkeuken1/data/post_mortem/masks.pandas')\n",
    "\n",
    "mask_stains = ['PARV', 'SMI32']\n",
    "raters_a = ['KH', 'MT']\n",
    "\n",
    "# There were a few masks missing (either due to not correct saving or skipping), so MCKeuken and AAlkemade parcellated the \n",
    "# remaing ones\n",
    "raters_b = ['MCK', 'AA']\n",
    "\n",
    "# A for loop that creates the .HDF5 files per tissue block \n",
    "for subject_id, d in df.groupby(['subject_id']):\n",
    "    print subject_id\n",
    "    \n",
    "    slices = natsort.natsorted(d.slice.unique())\n",
    "    \n",
    "    print slices\n",
    "    \n",
    "    stains = natsort.natsorted(d.stain.unique())\n",
    "    resolution = ndimage.imread(d.fn.iloc[0]).shape\n",
    "\n",
    "    data_array = np.zeros((len(slices),) + resolution + (len(stains),))\n",
    "    data_array[:] = np.nan\n",
    "    \n",
    "    print 'Storing data'\n",
    "    for idx, row in d.iterrows():\n",
    "        \n",
    "        slice_idx = slices.index(row['slice'])\n",
    "        stain_idx = stains.index(row['stain'])\n",
    "        \n",
    "        data_array[slice_idx, ..., stain_idx] = ndimage.imread(row.fn)\n",
    "        \n",
    "    mask_array = np.zeros((len(slices),) + resolution + (4,))\n",
    "    \n",
    "    print 'Storing masks'\n",
    "    for idx, row in masks.ix[subject_id].reset_index().iterrows():\n",
    "        \n",
    "        slice_idx = slices.index(row['slice'])\n",
    "        \n",
    "        if row.rater in raters_a:\n",
    "            last_idx = mask_stains.index(row.stain) * 2 + raters_a.index(row.rater)\n",
    "        else:\n",
    "            last_idx = mask_stains.index(row.stain) * 2 + raters_b.index(row.rater)\n",
    "        \n",
    "        im = ndimage.imread(row.fn)\n",
    "        mask_array[slice_idx, ..., last_idx] = im > np.percentile(im, 70)\n",
    "        \n",
    "        \n",
    "    print 'Creating HDF5 file'\n",
    "    p = '/home/mkeuken1/data/post_mortem/new_data_format/%s/' % subject_id\n",
    "    \n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)\n",
    "    \n",
    "    new_file = h5py.File(os.path.join(p, 'images.hdf5' % subject_id), )\n",
    "    new_file.create_dataset('data', data=data_array)\n",
    "    new_file.create_dataset('mask', data=mask_array.astype(bool))\n",
    "    new_file.close()\n",
    "    \n",
    "    d.to_pickle(os.path.join(p, 'data.pandas'))\n",
    "    masks.ix[subject_id].reset_index().to_pickle(os.path.join(p, 'masks.pandas'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Plot the data:\n",
    "#### There are two different types of plots that we are going for here. The first type is a plot that displays the intensity histogram of the stain which is combined with a tri-planner view of the STN. This is done per subject and stain. The second type of plot is used to check whether the MRI data aligns with the blockface images, whether the stains align with the blockface images, and finally whether the masks of the STN are located in a plausible location. \n",
    "\n",
    "#### It should be noted that we are not using the intensity per pixel but that we smooth the data a bit. Namely with a Gaussian smoothing kernel 0.3mm fwhm. For the original analysis we also used 0.15mm fwhm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "Plotting CALR\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting FER\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GABRA3\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GAD6567\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting MBP\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting PARV\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SERT\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SMI32\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SYN\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting TH\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting TRANSF\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting VGLUT1\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "Plotting CALR\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting FER\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GABRA3\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GAD6567\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting MBP\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting PARV\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SERT\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SMI32\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SYN\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting TH\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting TRANSF\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting VGLUT1\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "calculating vmin\n",
      "calculating vmax\n",
      "Plotting CALR\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting FER\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GABRA3\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting GAD6567\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting MBP\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting PARV\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SERT\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SMI32\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "Plotting SYN\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n",
      "(2292, 1944) (2292, 1944)\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# How does the data look like?\n",
    "############\n",
    "# To visualize the data we plot the stacked stains in a tri-planner view. This allows us to check whether there\n",
    "#   are slices that are still completely misaligned. \n",
    "# We also create an intensity histogram to get an initial feeling for how the data distribution looks like.\n",
    "#\n",
    "# Given the high resolution of the data and that we are interested in the distribution thoughout the STN we decided \n",
    "#   to smooth the data a bit. Either with a [0.15, 0.3, 0.6, 1.2, 2.4] mm fwhm Gaussian kernel. \n",
    "#\n",
    "############\n",
    "# Start code\n",
    "############\n",
    "#\n",
    "# Importing a number of different tools\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pystain import StainDataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Which tissue blocks are we going to visualize? \n",
    "subject_ids = [13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "\n",
    "# Ensure that the color coding is normalized between the min and max per stain\n",
    "def cmap_hist(data, bins=None, cmap=plt.cm.hot, vmin=None, vmax=None):\n",
    "    n, bins, patches = plt.hist(data, bins=bins)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    \n",
    "    if vmin is None:\n",
    "        vmin = data.min()\n",
    "    if vmax is None:\n",
    "        vmax = data.max()\n",
    "\n",
    "    # scale values to interval [0,1]\n",
    "    col = (bin_centers - vmin) / vmax\n",
    "\n",
    "    for c, p in zip(col, patches):\n",
    "        plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "# Create the figures per stain, per tissue block, per smoothing kernel [0.15, 0.3, 0.6, 1.2, 2.4].\n",
    "for subject_id in subject_ids[:]:\n",
    "    for fwhm in [0.15, 0.3, 0.6, 1.2, 2.4]:\n",
    "        dataset = StainDataset(subject_id, fwhm=fwhm)\n",
    "        dataset.get_vminmax((0, 99))\n",
    "\n",
    "        d = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/%s/' % (subject_id)\n",
    "        \n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d) \n",
    "\n",
    "        fn = os.path.join(d, 'stains_%s.pdf' % fwhm)\n",
    "        if os.path.isfile(fn):\n",
    "            continue\n",
    "        pdf = PdfPages(fn)\n",
    "        \n",
    "        for i, stain in enumerate(dataset.stains):\n",
    "            print 'Plotting %s' % stain\n",
    "            plt.figure()\n",
    "            # thresholded mask area is where at least 3 masks overlay\n",
    "            data = dataset.smoothed_data.value[dataset.thresholded_mask, i]\n",
    "            data = data[~np.isnan(data)]\n",
    "            bins = np.linspace(0, dataset.vmax[i], 100)\n",
    "            cmap_hist(data, bins, plt.cm.hot, vmin=dataset.vmin[i], vmax=dataset.vmax[i])\n",
    "            plt.title(stain)\n",
    "            plt.savefig(pdf, format='pdf')\n",
    "\n",
    "            plt.close(plt.gcf())\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            if not os.path.exists(d):\n",
    "                os.makedirs(d)\n",
    "\n",
    "            for i, orientation in enumerate(['coronal', 'axial', 'sagittal']):\n",
    "                for j, q in enumerate([.25, .5, .75]):\n",
    "                    ax = plt.subplot(3, 3, i + j*3 + 1)\n",
    "                    slice = dataset.get_proportional_slice(q, orientation)\n",
    "                    dataset.plot_slice(slice=slice, stain=stain, orientation=orientation, cmap=plt.cm.hot)\n",
    "                    ax.set_anchor('NW')\n",
    "\n",
    "            plt.gcf().set_size_inches(20, 20)\n",
    "            plt.suptitle(stain)\n",
    "            plt.savefig(pdf, format='pdf')\n",
    "            plt.close(plt.gcf())\n",
    "\n",
    "        pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3) Statistical analysis of the 27 PCA sectors\n",
    "3a) For each subject the data is collected, masked so that we only have the data in the masks, a two component PCA is run of which the first component is along the dorsal axis, whereas the second component is via the lateral axis. Then in the Y direction, or anterior/posterior axis, the structure is devided into three parts. Afterwards, for the lateral and dorsal PCA components, the line is devided into 3 parts. This is done for each Y slices, resulting in 3x3x3: 27 sectors. \n",
    "\n",
    "The data of those 27 sectors are then combined across subjects per stain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "[[-0.98094749 -0.19427308]\n",
      " [-0.19427308  0.98094749]]\n",
      "[[ 0.98094749  0.19427308]\n",
      " [ 0.19427308 -0.98094749]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "[[-0.95611755 -0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "[[ 0.95611755  0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "[[-0.78933812 -0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "[[ 0.78933812  0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "[[-0.70764237 -0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "[[ 0.70764237  0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "[[-0.66358108 -0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "[[ 0.66358108  0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "[[-0.61137631 -0.79134   ]\n",
      " [-0.79134     0.61137631]]\n",
      "[[ 0.61137631  0.79134   ]\n",
      " [ 0.79134    -0.61137631]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "[[-0.77494798 -0.63202503]\n",
      " [-0.63202503  0.77494798]]\n",
      "[[ 0.77494798  0.63202503]\n",
      " [ 0.63202503 -0.77494798]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "[[-0.98094749 -0.19427308]\n",
      " [-0.19427308  0.98094749]]\n",
      "[[ 0.98094749  0.19427308]\n",
      " [ 0.19427308 -0.98094749]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "[[-0.95611755 -0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "[[ 0.95611755  0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "[[-0.78933812 -0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "[[ 0.78933812  0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "[[-0.70764237 -0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "[[ 0.70764237  0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "[[-0.66358108 -0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "[[ 0.66358108  0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "[[-0.61137631 -0.79134   ]\n",
      " [-0.79134     0.61137631]]\n",
      "[[ 0.61137631  0.79134   ]\n",
      " [ 0.79134    -0.61137631]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "[[-0.77494798 -0.63202503]\n",
      " [-0.63202503  0.77494798]]\n",
      "[[ 0.77494798  0.63202503]\n",
      " [ 0.63202503 -0.77494798]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "[[-0.98094749 -0.19427308]\n",
      " [-0.19427308  0.98094749]]\n",
      "[[ 0.98094749  0.19427308]\n",
      " [ 0.19427308 -0.98094749]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "[[-0.95611755 -0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "[[ 0.95611755  0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "[[-0.78933812 -0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "[[ 0.78933812  0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "[[-0.70764237 -0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "[[ 0.70764237  0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "[[-0.66358108 -0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "[[ 0.66358108  0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "[[-0.61137631 -0.79134   ]\n",
      " [-0.79134     0.61137631]]\n",
      "[[ 0.61137631  0.79134   ]\n",
      " [ 0.79134    -0.61137631]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "[[-0.77494798 -0.63202503]\n",
      " [-0.63202503  0.77494798]]\n",
      "[[ 0.77494798  0.63202503]\n",
      " [ 0.63202503 -0.77494798]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "[[-0.98094749 -0.19427308]\n",
      " [-0.19427308  0.98094749]]\n",
      "[[ 0.98094749  0.19427308]\n",
      " [ 0.19427308 -0.98094749]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "[[-0.95611755 -0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "[[ 0.95611755  0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "[[-0.78933812 -0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "[[ 0.78933812  0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "[[-0.70764237 -0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "[[ 0.70764237  0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "[[-0.66358108 -0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "[[ 0.66358108  0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "[[-0.61137631 -0.79134   ]\n",
      " [-0.79134     0.61137631]]\n",
      "[[ 0.61137631  0.79134   ]\n",
      " [ 0.79134    -0.61137631]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "[[-0.77494798 -0.63202503]\n",
      " [-0.63202503  0.77494798]]\n",
      "[[ 0.77494798  0.63202503]\n",
      " [ 0.63202503 -0.77494798]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "[[-0.98094749 -0.19427308]\n",
      " [-0.19427308  0.98094749]]\n",
      "[[ 0.98094749  0.19427308]\n",
      " [ 0.19427308 -0.98094749]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "[[-0.95611755 -0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "[[ 0.95611755  0.29298334]\n",
      " [ 0.29298334 -0.95611755]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "[[-0.78933812 -0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "[[ 0.78933812  0.61395874]\n",
      " [ 0.61395874 -0.78933812]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "[[-0.70764237 -0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "[[ 0.70764237  0.70657079]\n",
      " [ 0.70657079 -0.70764237]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "[[-0.66358108 -0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "[[ 0.66358108  0.74810437]\n",
      " [ 0.74810437 -0.66358108]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "[[-0.61137631 -0.79134   ]\n",
      " [-0.79134     0.61137631]]\n",
      "[[ 0.61137631  0.79134   ]\n",
      " [ 0.79134    -0.61137631]]\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "[[-0.77494798 -0.63202503]\n",
      " [-0.63202503  0.77494798]]\n",
      "[[ 0.77494798  0.63202503]\n",
      " [ 0.63202503 -0.77494798]]\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Is the data uniformly distributed over the STN?\n",
    "############\n",
    "#\n",
    "# To test this question we divide the STN into 27 sectors based on a PCA analysis where we identify the three main \n",
    "#   axes which are then each divided into three parts. \n",
    "#\n",
    "# The mean intensity per stain is subtracted of each elipsoid, so that if the data is uniformly distributed each\n",
    "#   sector would be equal to zero. If there are sectors that have a signal lower than the overall mean these sectors\n",
    "#   will have a negative value and vice versa for higher signals. \n",
    "# \n",
    "\n",
    "# Importing a number of different tools\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "import pandas\n",
    "from pystain import StainDataset\n",
    "\n",
    "# Start code\n",
    "subject_id = 13095\n",
    "ds = StainDataset(subject_id)\n",
    "\n",
    "conversion_matrix = np.array([[0, 0, ds.xy_resolution],\n",
    "                      [-ds.z_resolution, 0, 0],\n",
    "                      [0, -ds.xy_resolution, 0]])\n",
    "results = []\n",
    "\n",
    "# What are the subject IDs?\n",
    "subject_ids=[13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "\n",
    "# What fwhm to use here?\n",
    "fwhms = [0.15, 0.3, 0.6, 1.2, 2.4]\n",
    "for fwhm in fwhms:\n",
    "    for subject_id in subject_ids[:]:\n",
    "        ds = StainDataset(subject_id, fwhm=fwhm)\n",
    "\n",
    "    # Get coordinates of mask and bring them to mm\n",
    "        x, y, z = np.where(ds.thresholded_mask)\n",
    "        coords = np.column_stack((x, y, z))\n",
    "        coords_mm = conversion_matrix.dot(coords.T).T\n",
    "        coords_mm -= coords_mm.mean(0)\n",
    "\n",
    "    # Fit two components and make sure first axis walks dorsal\n",
    "    #   and second component lateral\n",
    "        pca = PCA()\n",
    "        pca.fit_transform((coords_mm - coords_mm.mean(0))[:, (0, 2)])\n",
    "\n",
    "        components = pca.components_\n",
    "        print components\n",
    "\n",
    "        if components[0, 1] < 0:\n",
    "            components[0] = -components[0]\n",
    "\n",
    "        if components[1, 0] < 0:\n",
    "            components[1] = -components[1]\n",
    "\n",
    "        print components\n",
    "\n",
    "        coords_dataframe = pandas.DataFrame(coords_mm, columns=['x_mm', 'y_mm', 'z_mm'])\n",
    "        coords_dataframe['slice'] = x\n",
    "\n",
    "        coords_dataframe['pc1'] = components.dot(coords_mm[:, (0, 2)].T)[0, :]\n",
    "        coords_dataframe['pc2'] = components.dot(coords_mm[:, (0, 2)].T)[1, :]\n",
    "\n",
    "        coords_dataframe[['pc1_slice_center', 'pc2_slice_center']] = coords_dataframe.groupby(['slice'])[['pc1', 'pc2']].apply(lambda x: x - x.mean())\n",
    "\n",
    "        coords_dataframe['slice_3'] = pandas.qcut(coords_dataframe.y_mm, 3, labels=['posterior', 'middle', 'anterior'])    \n",
    "\n",
    "        coords_dataframe['pc1_3'] = coords_dataframe.groupby('slice_3').pc1.apply(lambda d: pandas.qcut(d, 3, labels=['ventral', 'middle', 'dorsal']))\n",
    "        coords_dataframe['pc2_3'] = coords_dataframe.groupby(['slice_3', 'pc1_3']).pc2.apply(lambda d: pandas.qcut(d, 3, labels=['medial', 'middle', 'lateral']))\n",
    "\n",
    "        df= pandas.concat((ds.smoothed_dataframe, coords_dataframe), 1)\n",
    "        tmp = df.pivot_table(index=['pc1_3', 'pc2_3', 'slice_3'], values=ds.stains, aggfunc='mean').copy()\n",
    "        tmp['subject_id'] = subject_id\n",
    "        tmp['fwhm'] = fwhm\n",
    "\n",
    "        results.append(tmp.copy())\n",
    "\n",
    "df = pandas.concat(results).reset_index().set_index(['subject_id', 'slice_3', 'pc1_3', 'pc2_3'])\n",
    "df = pandas.melt(df.reset_index(), id_vars=['fwhm', 'subject_id', 'slice_3', 'pc1_3', 'pc2_3'], var_name='stain')\n",
    "df['value'] = df.groupby(['fwhm', 'subject_id', 'stain']).transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Statistical analysis of the 27 PCA sectors\n",
    "#### 3b) For each stain and sector we do a simple t-test to compare whether the intensity values are different from zero. This is corrected for multiple comparisons using a fdr correction, critical p-value of 0.05.\n",
    "\n",
    "#### The sectors that survive the fdr correction are then plotted on the elipsoid, where red indicates above average intensity, blue indicates below average intensity. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matplotlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-55d1241e1915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# gray 'background' of STN instead of white\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSegmentedColormap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'colormap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lightgray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_ellipse_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipse_pars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoolwarm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matplotlib' is not defined"
     ]
    }
   ],
   "source": [
    "# this first half plots everything in a separate pdf-file for each stain\n",
    "\n",
    "# # Importing a number of different tools\n",
    "# from statsmodels.sandbox.stats import multicomp\n",
    "# from matplotlib import patches\n",
    "# import scipy as sp\n",
    "# sns.set_style('white')\n",
    "# df.stain.unique()\n",
    "\n",
    "# def plot_ellipse_values(values, ellipse_pars=None, size=(1000, 1000), vmin=None, vmax=None, cmap=plt.cm.coolwarm, **kwargs):\n",
    "\n",
    "#     ''' values is a n-by-m array'''\n",
    "\n",
    "#     if ellipse_pars is None:\n",
    "#         a = 350\n",
    "#         b = 150\n",
    "#         x = 500\n",
    "#         y = 500\n",
    "\n",
    "#         theta = 45. / 180 * np.pi\n",
    "\n",
    "#     else:\n",
    "#         a, b, x, y, theta = ellipse_pars\n",
    "\n",
    "#     A = a**2 * (np.sin(theta))**2 + b**2 * (np.cos(theta))**2\n",
    "#     B = 2 * (b**2 - a**2) * np.sin(theta) * np.cos(theta)\n",
    "#     C = a**2 * np.cos(theta)**2 + b**2 * np.sin(theta)**2\n",
    "#     D = -2 * A * x - B* y\n",
    "#     E = -B * x - 2 * C * y\n",
    "#     F = A* x**2 + B*x*y + C*y**2 - a**2*b**2\n",
    "\n",
    "#     X,Y = np.meshgrid(np.arange(size[0]), np.arange(size[1]))\n",
    "\n",
    "#     in_ellipse = A*X**2 + B*X*Y +C*Y**2 + D*X + E*Y +F < 0\n",
    "\n",
    "#     pc1 = np.array([[np.cos(theta)], [np.sin(theta)]])\n",
    "#     pc2 = np.array([[np.cos(theta - np.pi/2.)], [np.sin(theta - np.pi/2.)]])\n",
    "\n",
    "#     pc1_distance = pc1.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "#     pc2_distance = pc2.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "\n",
    "#     pc1_quantile = np.floor((pc1_distance / a + 1 ) / 2. * values.shape[0])\n",
    "#     pc2_quantile = np.floor((pc2_distance / b + 1 ) / 2. * values.shape[1])\n",
    "\n",
    "#     im = np.zeros_like(X, dtype=float)\n",
    "\n",
    "#     for pc1_q in np.arange(values.shape[0]):\n",
    "#         for pc2_q in np.arange(values.shape[1]):\n",
    "#             im[in_ellipse * (pc1_quantile == pc1_q) & (pc2_quantile == pc2_q)] = values[pc1_q, pc2_q]\n",
    "\n",
    "#     im = np.ma.masked_array(im, ~in_ellipse)\n",
    "#     cax = plt.imshow(im, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\n",
    "#     sns.despine()\n",
    "\n",
    "#     return cax\n",
    "\n",
    "# fwhms = [0.15, 0.3, 0.6, 1.2, 2.4]\n",
    "# for fwhm in fwhms:\n",
    "#     # What is the output folder for the PCA figures:\n",
    "#     pca_folder = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/PCA_sectors/fwhm_%s' %fwhm\n",
    "#     if not os.path.exists(pca_folder):\n",
    "#         os.makedirs(pca_folder) \n",
    "\n",
    "#     # For every stain and sector over the 7 subjects plot the data and test whether it differs from zero:\n",
    "#     for stain, d in df.loc[df.fwhm==fwhm].groupby(['stain']):\n",
    "#         fn = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/PCA_sectors/fwhm_%s/%s_big_picture_coolwarm.pdf' %(fwhm, stain)\n",
    "#         pdf = PdfPages(fn)\n",
    "\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "#         for i, (slice, d2) in enumerate(d.groupby('slice_3')):\n",
    "\n",
    "#             ax = plt.subplot(1, 3, ['anterior', 'middle', 'posterior'].index(slice) + 1)\n",
    "\n",
    "#             n = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: len(v)).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "#             t = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: sp.stats.ttest_1samp(v, 0,nan_policy='omit')[0]).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "#             p = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: sp.stats.ttest_1samp(v, 0,nan_policy='omit')[1]).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "#             mean = d2.groupby(['pc1_3', 'pc2_3']).value.mean().unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "\n",
    "#             # FDR: as we are doing 27 seperate t-tests we need to correct for multiple comparisons:\n",
    "#             p.values[:] = multicomp.fdrcorrection0(p.values.ravel())[1].reshape(3, 3)\n",
    "\n",
    "#             # Providing some parameters for plotting the figures\n",
    "#             if i == 1:\n",
    "#                 a, b, x, y, theta  = 350, 150, 300, 275, 45\n",
    "#             else:\n",
    "#                 a, b, x, y, theta  = 300, 125, 300, 275, 45.\n",
    "\n",
    "#             plot_ellipse_values(t[p<0.05].values, size=(600, 550), ellipse_pars=(a, b, x, y,  theta / 180. * np.pi), vmin=-7, vmax=7, cmap=plt.cm.coolwarm)\n",
    "\n",
    "#             e1 = patches.Ellipse((x, y), a*2, b*2,\n",
    "#                              angle=theta, linewidth=2, fill=False, zorder=2)\n",
    "\n",
    "#             ax.add_patch(e1)\n",
    "\n",
    "#             plt.xticks([])\n",
    "#             plt.yticks([])    \n",
    "\n",
    "#             sns.despine(bottom=True, left=True)\n",
    "\n",
    "#             print stain\n",
    "#             print p.values  \n",
    "#         plt.suptitle(stain, fontsize=24)\n",
    "#         fig.set_size_inches(15., 4.)\n",
    "#         pdf.savefig(fig, transparent=True)    \n",
    "#         pdf.close()\n",
    "\n",
    "\n",
    "\n",
    "# Plot all stains together in a single figure\n",
    "# Importing a number of different tools\n",
    "from statsmodels.sandbox.stats import multicomp\n",
    "from matplotlib import patches\n",
    "import scipy as sp\n",
    "sns.set_style('white')\n",
    "df.stain.unique()\n",
    "%matplotlib inline\n",
    "\n",
    "# gray 'background' of STN instead of white\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('colormap', ['blue', 'lightgray', 'red'])\n",
    "def plot_ellipse_values(values, ellipse_pars=None, size=(1000, 1000), vmin=None, vmax=None, cmap=plt.cm.coolwarm, ax=None, **kwargs):\n",
    "\n",
    "    ''' values is a n-by-m array'''\n",
    "\n",
    "    values[np.isnan(values)] = 0\n",
    "    if ellipse_pars is None:\n",
    "        a = 350\n",
    "        b = 150\n",
    "        x = 500\n",
    "        y = 500\n",
    "\n",
    "        theta = 45. / 180 * np.pi\n",
    "\n",
    "    else:\n",
    "        a, b, x, y, theta = ellipse_pars\n",
    "\n",
    "    A = a**2 * (np.sin(theta))**2 + b**2 * (np.cos(theta))**2\n",
    "    B = 2 * (b**2 - a**2) * np.sin(theta) * np.cos(theta)\n",
    "    C = a**2 * np.cos(theta)**2 + b**2 * np.sin(theta)**2\n",
    "    D = -2 * A * x - B* y\n",
    "    E = -B * x - 2 * C * y\n",
    "    F = A* x**2 + B*x*y + C*y**2 - a**2*b**2\n",
    "\n",
    "    X,Y = np.meshgrid(np.arange(size[0]), np.arange(size[1]))\n",
    "\n",
    "    in_ellipse = A*X**2 + B*X*Y +C*Y**2 + D*X + E*Y +F < 0\n",
    "\n",
    "    pc1 = np.array([[np.cos(theta)], [np.sin(theta)]])\n",
    "    pc2 = np.array([[np.cos(theta - np.pi/2.)], [np.sin(theta - np.pi/2.)]])\n",
    "\n",
    "    pc1_distance = pc1.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "    pc2_distance = pc2.T.dot(np.array([(X - x).ravel(), (Y - y).ravel()])).reshape(X.shape)\n",
    "\n",
    "    pc1_quantile = np.floor((pc1_distance / a + 1 ) / 2. * values.shape[0])\n",
    "    pc2_quantile = np.floor((pc2_distance / b + 1 ) / 2. * values.shape[1])\n",
    "\n",
    "    im = np.zeros_like(X, dtype=float)\n",
    "\n",
    "    for pc1_q in np.arange(values.shape[0]):\n",
    "        for pc2_q in np.arange(values.shape[1]):\n",
    "            im[in_ellipse * (pc1_quantile == pc1_q) & (pc2_quantile == pc2_q)] = values[pc1_q, pc2_q]\n",
    "\n",
    "    im = np.ma.masked_array(im, ~in_ellipse)\n",
    "#     cmap.set_bad('grey')\n",
    "    if ax is None:\n",
    "        cax = plt.imshow(im, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\n",
    "    else:\n",
    "        ax.imshow(im, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\n",
    "        cax = ax\n",
    "#    sns.despine()\n",
    "\n",
    "    return cax\n",
    "\n",
    "fwhms = [0.3, 0.6, 1.2, 2.4]\n",
    "for fwhm in fwhms:\n",
    "    # What is the output folder for the PCA figures:\n",
    "    pca_folder = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/PCA_sectors/fwhm_%s' %fwhm\n",
    "    if not os.path.exists(pca_folder):\n",
    "        os.makedirs(pca_folder) \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=3*3)\n",
    "    \n",
    "    # For every stain and sector over the 7 subjects plot the data and test whether it differs from zero:\n",
    "    for ii, (stain, d) in enumerate(df.loc[df.fwhm==fwhm].groupby(['stain'])):\n",
    "        fn = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/PCA_sectors/fwhm_%s/combined_big_picture_coolwarm.pdf' %(fwhm)\n",
    "#        pdf = PdfPages(fn)\n",
    "        column_set = int(np.floor(ii/4.))\n",
    "        row_n = (ii)%4.\n",
    "\n",
    "        for i, (slice, d2) in enumerate(d.groupby('slice_3')):\n",
    "            print(row_n, ['anterior', 'middle', 'posterior'].index(slice) + 3*(column_set))\n",
    "            ax = axes[row_n, ['anterior', 'middle', 'posterior'].index(slice) + 3*(column_set)]\n",
    "#            ax = plt.subplot(1, 3, ['anterior', 'middle', 'posterior'].index(slice) + 1)\n",
    "\n",
    "            n = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: len(v)).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "            t = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: sp.stats.ttest_1samp(v, 0,nan_policy='omit')[0]).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "            p = d2.groupby(['pc1_3', 'pc2_3']).value.apply(lambda v: sp.stats.ttest_1samp(v, 0,nan_policy='omit')[1]).unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "            mean = d2.groupby(['pc1_3', 'pc2_3']).value.mean().unstack(1).ix[['ventral', 'middle', 'dorsal'], ['medial', 'middle', 'lateral']]\n",
    "\n",
    "            # FDR: as we are doing 27 seperate t-tests we need to correct for multiple comparisons:\n",
    "            p.values[:] = multicomp.fdrcorrection0(p.values.ravel())[1].reshape(3, 3)\n",
    "\n",
    "            # Providing some parameters for plotting the figures\n",
    "            if i == 1:\n",
    "                a, b, x, y, theta  = 350, 150, 300, 275, 45\n",
    "            else:\n",
    "                a, b, x, y, theta  = 300, 125, 300, 275, 45.\n",
    "\n",
    "            plot_ellipse_values(t[p<0.05].values, size=(600, 550), ellipse_pars=(a, b, x, y,  theta / 180. * np.pi), vmin=-7, vmax=7, cmap=cmap, ax=ax)\n",
    "\n",
    "            e1 = patches.Ellipse((x, y), a*2, b*2,\n",
    "                                 angle=theta, linewidth=2, fill=False, zorder=2)\n",
    "\n",
    "            ax.add_patch(e1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            sns.despine(bottom=True, left=True, right=True)\n",
    "            \n",
    "            if slice == 'middle':\n",
    "                ax.set_title(stain, fontsize=24)\n",
    "            \n",
    "            print stain\n",
    "            print p.values\n",
    "\n",
    "    fig.set_size_inches(10.*2, 4.*2)\n",
    "    fig.subplots_adjust(hspace=.275, wspace=0.00, bottom=0.01, left=0.0, top=.95, right=1)\n",
    "    \n",
    "    plt.plot([0, 0], [0, 1], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "    plt.plot([0, 1], [1, 1], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "    plt.plot([1, 1], [0, 1], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "    plt.plot([0, 1], [0, 0], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "\n",
    "    plt.plot([1/3., 1/3.], [0, 1], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "    plt.plot([2/3., 2/3.], [0, 1], color='black', lw=1, transform=plt.gcf().transFigure, clip_on=False)\n",
    "#    fig.savefig(pdf,  format='pdf')#, bbox_inches='tight')\n",
    "#    pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Mixture analysis\n",
    "   The plan is to fit a mixture of exGausian distributions to either the intensity or gradient vector data per stain. \n",
    "    The idea here is that through model comparison we can then figure out whether the data is better explained by a \n",
    "        single exGauss or by a mixture of maximum 6 exGuassians. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a1) First we need to create the feature vectors which we are going to use for the model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "All slices available for stain GAD6567!\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "All slices available for stain PARV!\n",
      " *** SERT ***\n",
      "All slices available for stain SERT!\n",
      " *** SMI32 ***\n",
      "All slices available for stain SMI32!\n",
      " *** SYN ***\n",
      "All slices available for stain SYN!\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 300 (can NOT be interpolated)\n",
      " * slice 500 (can NOT be interpolated)\n",
      " * slice 550 (can NOT be interpolated)\n",
      " * slice 1150 (can be interpolated)\n",
      "float64 (2292, 1944)\n",
      "float64 (27, 2292, 1944, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "All slices available for stain GAD6567!\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "All slices available for stain PARV!\n",
      " *** SERT ***\n",
      "All slices available for stain SERT!\n",
      " *** SMI32 ***\n",
      "All slices available for stain SMI32!\n",
      " *** SYN ***\n",
      "All slices available for stain SYN!\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 300 (can NOT be interpolated)\n",
      " * slice 500 (can NOT be interpolated)\n",
      " * slice 550 (can NOT be interpolated)\n",
      " * slice 1150 (can be interpolated)\n",
      "float64 (2292, 1944)\n",
      "float64 (27, 2292, 1944, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "All slices available for stain GAD6567!\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "All slices available for stain PARV!\n",
      " *** SERT ***\n",
      "All slices available for stain SERT!\n",
      " *** SMI32 ***\n",
      "All slices available for stain SMI32!\n",
      " *** SYN ***\n",
      "All slices available for stain SYN!\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 300 (can NOT be interpolated)\n",
      " * slice 500 (can NOT be interpolated)\n",
      " * slice 550 (can NOT be interpolated)\n",
      " * slice 1150 (can be interpolated)\n",
      "float64 (2292, 1944)\n",
      "float64 (27, 2292, 1944, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "All slices available for stain GAD6567!\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "All slices available for stain PARV!\n",
      " *** SERT ***\n",
      "All slices available for stain SERT!\n",
      " *** SMI32 ***\n",
      "All slices available for stain SMI32!\n",
      " *** SYN ***\n",
      "All slices available for stain SYN!\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 300 (can NOT be interpolated)\n",
      " * slice 500 (can NOT be interpolated)\n",
      " * slice 550 (can NOT be interpolated)\n",
      " * slice 1150 (can be interpolated)\n",
      "float64 (2292, 1944)\n",
      "float64 (27, 2292, 1944, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "All slices available for stain GAD6567!\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "All slices available for stain PARV!\n",
      " *** SERT ***\n",
      "All slices available for stain SERT!\n",
      " *** SMI32 ***\n",
      "All slices available for stain SMI32!\n",
      " *** SYN ***\n",
      "All slices available for stain SYN!\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 300 (can NOT be interpolated)\n",
      " * slice 500 (can NOT be interpolated)\n",
      " * slice 550 (can NOT be interpolated)\n",
      " * slice 1150 (can be interpolated)\n",
      "float64 (2292, 1944)\n",
      "float64 (27, 2292, 1944, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "Slices that are not available for stain GAD6567:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " * slice 2050 (can NOT be interpolated)\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "Slices that are not available for stain PARV:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SERT ***\n",
      "Slices that are not available for stain SERT:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SMI32 ***\n",
      "Slices that are not available for stain SMI32:\n",
      " * slice 1400 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** SYN ***\n",
      "Slices that are not available for stain SYN:\n",
      " * slice 1800 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 1250 (can NOT be interpolated)\n",
      " * slice 1300 (can NOT be interpolated)\n",
      " * slice 1350 (can NOT be interpolated)\n",
      " * slice 1400 (can NOT be interpolated)\n",
      " * slice 1450 (can NOT be interpolated)\n",
      " * slice 1500 (can NOT be interpolated)\n",
      " * slice 1700 (can NOT be interpolated)\n",
      " * slice 1750 (can NOT be interpolated)\n",
      " * slice 1950 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "Slices that are not available for stain GAD6567:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " * slice 2050 (can NOT be interpolated)\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "Slices that are not available for stain PARV:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SERT ***\n",
      "Slices that are not available for stain SERT:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SMI32 ***\n",
      "Slices that are not available for stain SMI32:\n",
      " * slice 1400 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** SYN ***\n",
      "Slices that are not available for stain SYN:\n",
      " * slice 1800 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 1250 (can NOT be interpolated)\n",
      " * slice 1300 (can NOT be interpolated)\n",
      " * slice 1350 (can NOT be interpolated)\n",
      " * slice 1400 (can NOT be interpolated)\n",
      " * slice 1450 (can NOT be interpolated)\n",
      " * slice 1500 (can NOT be interpolated)\n",
      " * slice 1700 (can NOT be interpolated)\n",
      " * slice 1750 (can NOT be interpolated)\n",
      " * slice 1950 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "Slices that are not available for stain GAD6567:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " * slice 2050 (can NOT be interpolated)\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "Slices that are not available for stain PARV:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SERT ***\n",
      "Slices that are not available for stain SERT:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SMI32 ***\n",
      "Slices that are not available for stain SMI32:\n",
      " * slice 1400 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** SYN ***\n",
      "Slices that are not available for stain SYN:\n",
      " * slice 1800 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 1250 (can NOT be interpolated)\n",
      " * slice 1300 (can NOT be interpolated)\n",
      " * slice 1350 (can NOT be interpolated)\n",
      " * slice 1400 (can NOT be interpolated)\n",
      " * slice 1450 (can NOT be interpolated)\n",
      " * slice 1500 (can NOT be interpolated)\n",
      " * slice 1700 (can NOT be interpolated)\n",
      " * slice 1750 (can NOT be interpolated)\n",
      " * slice 1950 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      " *** CALR ***\n",
      "All slices available for stain CALR!\n",
      " *** FER ***\n",
      "All slices available for stain FER!\n",
      " *** GABRA3 ***\n",
      "All slices available for stain GABRA3!\n",
      " *** GAD6567 ***\n",
      "Slices that are not available for stain GAD6567:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " * slice 2050 (can NOT be interpolated)\n",
      " *** MBP ***\n",
      "All slices available for stain MBP!\n",
      " *** PARV ***\n",
      "Slices that are not available for stain PARV:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SERT ***\n",
      "Slices that are not available for stain SERT:\n",
      " * slice 800 (can NOT be interpolated)\n",
      " *** SMI32 ***\n",
      "Slices that are not available for stain SMI32:\n",
      " * slice 1400 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** SYN ***\n",
      "Slices that are not available for stain SYN:\n",
      " * slice 1800 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TH ***\n",
      "Slices that are not available for stain TH:\n",
      " * slice 1250 (can NOT be interpolated)\n",
      " * slice 1300 (can NOT be interpolated)\n",
      " * slice 1350 (can NOT be interpolated)\n",
      " * slice 1400 (can NOT be interpolated)\n",
      " * slice 1450 (can NOT be interpolated)\n",
      " * slice 1500 (can NOT be interpolated)\n",
      " * slice 1700 (can NOT be interpolated)\n",
      " * slice 1750 (can NOT be interpolated)\n",
      " * slice 1950 (can be interpolated)\n",
      "float64 (1500, 1500)\n",
      "float64 (26, 1500, 1500, 12)\n",
      " *** TRANSF ***\n",
      "All slices available for stain TRANSF!\n",
      " *** VGLUT1 ***\n",
      "All slices available for stain VGLUT1!\n",
      "Saving intensity data to cross-validation partitioned dataframes...\n",
      "Saving gradient data to cross-validation partitioned dataframes...\n",
      "Saving gradient_2D data to cross-validation partitioned dataframes...\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "Calculcating gradient of CALR\n",
      "Calculcating gradient of FER\n",
      "Calculcating gradient of GABRA3\n",
      "Calculcating gradient of GAD6567\n",
      "Calculcating gradient of MBP\n",
      "Calculcating gradient of PARV\n",
      "Calculcating gradient of SERT\n",
      "Calculcating gradient of SMI32\n",
      "Calculcating gradient of SYN\n",
      "Calculcating gradient of TH\n",
      "Calculcating gradient of TRANSF\n",
      "Calculcating gradient of VGLUT1\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors\n",
    "# For both the intensity and the gradient vectors\n",
    "# These feature vectors are used to fit the exGaussian mixture models to. \n",
    "# This is done on three different test and training sets which are created in the cluster.py script\n",
    "#\n",
    "# Import several functions\n",
    "import glob\n",
    "import pystain\n",
    "from pystain import StainDataset, StainCluster\n",
    "\n",
    "# Get the data\n",
    "fns = glob.glob('/home/mkeuken1/data/post_mortem/new_data_format/*')\n",
    "\n",
    "# Get the subject id\n",
    "subject_ids = [int(fn.split('/')[-1]) for fn in fns]\n",
    "\n",
    "# For the subjects and stains create .pkl files (see cluster.py script)\n",
    "for subject_id in subject_ids:\n",
    "    for fwhm in [0.15, 0.3, 0.6, 1.2, 2.4]:\n",
    "        dataset = StainDataset(subject_id, fwhm=fwhm)\n",
    "        # If the data already exist, just grab it instead of recreating it again and then giving an error\n",
    "        #  that it already exists:\n",
    "        if 'data_smoothed_%s_thr_3' % str(fwhm) not in dataset.h5file.keys():\n",
    "            _ = StainDataset(subject_id, thr=3, fwhm=fwhm)\n",
    "        if 'data_smoothed_%s_thr_3_gradient_image' % str(fwhm) not in dataset.h5file.keys():\n",
    "            dataset.get_gradient_images() \n",
    "        if 'data_smoothed_%s_thr_3_gradient_image_2D' % str(fwhm) not in dataset.h5file.keys():\n",
    "            dataset.get_gradient_images_2D()\n",
    "\n",
    "        cluster = StainCluster(dataset, fwhm=fwhm)\n",
    "        cluster.make_feature_vector(save_directory_intensity='/home/mkeuken1/data/post_mortem/crossval_intensity_feature_vectors/', \n",
    "                                    save_directory_gradient='/home/mkeuken1/data/post_mortem/crossval_gradient_feature_vectors/',\n",
    "                                    save_directory_gradient_2D='/home/mkeuken1/data/post_mortem/crossval_gradient_2D_feature_vectors/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a2) Once we have the feature vectors we now want to fit the actual model onto the data and cross validate.\n",
    "    AIC and BIC will be done on the entire STN mask\n",
    "    \n",
    "    Cross validation will be done in two general ways:\n",
    "    1. within a subject, within a stain\n",
    "    2. betweeen a subject, within a stain\n",
    "    \n",
    "    For each given stain we have three different train and test sets:\n",
    "    \n",
    "    CV dataset 1: train on every odd slice, test on every even slice.\n",
    "    CV dataset 2: train on every even slice and test on every even slice\n",
    "                  Skip every second slice to decorrelate train and test set\n",
    "    CV dataset 3: train on every odd slice and test on every odd slice.\n",
    "                  Skip every second slice to decorrelate train and test set\n",
    "    For every given train and test set pair we also swap them around. So in total we have 6 different model cross \n",
    "                    validations. \n",
    "                    \n",
    "    Other things to note\n",
    "    - Using the entire dataset was computationally too heavy, therefore we sample 15% of the datapoints evenly spaced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is used to fit the mixture models to the data.\n",
    "# Note that this script has been ported to a .py file \n",
    "# The .py file was saved as fit_ml_cv_all_gradient_intensity_combined.py and run on lisa\n",
    "#    This means that there are a few pieces of code that are lisa (surfsara.nl) specific.\n",
    "\n",
    "# Import several functions and set a few style features:\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# How many processes do you want to run simulataneously on Lisa? The node has 16 cores, it is advised due\n",
    "#  to mememory issues you dont use all of them at the same time.\n",
    "n_proc = 14\n",
    "\n",
    "# Defining a number of functions:    \n",
    "def exgauss_pdf(x, mu, sigma, nu):\n",
    "\n",
    "    nu = 1./nu\n",
    "    p1 = nu / 2. * np.exp((nu/2.)  * (2 * mu + nu * sigma**2. - 2. * x))\n",
    "    p2 = sp.special.erfc((mu + nu * sigma**2 - x)/ (np.sqrt(2.) * sigma))\n",
    "\n",
    "    return p1 * p2\n",
    "\n",
    "def mixed_exgauss_likelihood(x, w, mu, sigma, nu):\n",
    "\n",
    "    # Create indiviudal\n",
    "    pdfs = w * exgauss_pdf(x[:, np.newaxis], mu, nu, sigma)\n",
    "    ll = np.sum(np.log(np.sum(pdfs, 1)))\n",
    "\n",
    "    if ((np.isnan(ll)) | (ll == np.inf)):\n",
    "        return -np.inf\n",
    "\n",
    "    return ll\n",
    "\n",
    "def input_optimizer(pars, x, n_clusters):\n",
    "\n",
    "    pars = np.array(pars)\n",
    "\n",
    "    if np.sum(pars[:n_clusters-1]) > 1:\n",
    "        return np.inf\n",
    "\n",
    "    pars = np.insert(pars, n_clusters-1, 1 - np.sum(pars[:n_clusters-1]))\n",
    "\n",
    "    if np.any(pars[:n_clusters] < 0.05):\n",
    "        return np.inf\n",
    "\n",
    "    w = pars[:n_clusters][np.newaxis, :]\n",
    "    mu = pars[n_clusters:n_clusters*2][np.newaxis, :]\n",
    "    nu = pars[n_clusters*2:n_clusters*3][np.newaxis, :]\n",
    "    sigma = pars[n_clusters*3:n_clusters*4][np.newaxis, :]\n",
    "\n",
    "    return -mixed_exgauss_likelihood(x, w, mu, sigma, nu)\n",
    "\n",
    "def _fit(input_args, disp=False, popsize=100, **kwargs):\n",
    "\n",
    "    sp.random.seed()\n",
    "\n",
    "    x, n_clusters = input_args\n",
    "\n",
    "    weight_bounds = [(1e-3, 1)] * (n_clusters - 1)\n",
    "    mu_bounds = [(-1., 2.5)] * n_clusters\n",
    "    nu_bounds = [(1e-3, 2.5)] * n_clusters\n",
    "    sigma_bounds = [(1e-3, 2.5)] * n_clusters\n",
    "\n",
    "    bounds = weight_bounds + mu_bounds + nu_bounds + sigma_bounds\n",
    "\n",
    "    result = sp.optimize.differential_evolution(input_optimizer, bounds, (x, n_clusters), polish=True, disp=disp, maxiter=500, popsize=popsize, **kwargs)\n",
    "    result = sp.optimize.minimize(input_optimizer, result.x, (x, n_clusters), method='SLSQP', bounds=bounds, **kwargs)\n",
    "\n",
    "    return result\n",
    "\n",
    "class SimpleExgaussMixture(object):\n",
    "\n",
    "    def __init__(self, data, n_clusters):\n",
    "\n",
    "        self.data = data\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_parameters = n_clusters * 4 - 1\n",
    "        self.likelihood = -np.inf\n",
    "\n",
    "        self.previous_likelihoods = []\n",
    "        self.previous_pars = []\n",
    "\n",
    "    def get_likelihood_data(self, data):\n",
    "        \n",
    "        return mixed_exgauss_likelihood(data, self.w, self.mu, self.sigma, self.nu)\n",
    "    \n",
    "    def get_bic_data(self, data):\n",
    "        likelihood = self.get_likelihood_data(data)\n",
    "        return - 2 * likelihood + self.n_parameters * np.log(data.shape[0])\n",
    "       \n",
    "    def get_aic_data(self, data):\n",
    "        likelihood = self.get_likelihood_data(data)\n",
    "        return 2 * self.n_parameters - 2  * likelihood\n",
    "    \n",
    "    def _fit(self, **kwargs):\n",
    "        return _fit((self.data, self.n_clusters), **kwargs)\n",
    "\n",
    "    def fit(self, n_tries=1, **kwargs):\n",
    "        for run in np.arange(n_tries):\n",
    "\n",
    "            result = self._fit(**kwargs)\n",
    "            self.previous_likelihoods.append(-result.fun)\n",
    "\n",
    "            if -result.fun > self.likelihood:\n",
    "\n",
    "                pars = result.x\n",
    "                pars = np.insert(pars, self.n_clusters-1, 1 - np.sum(pars[:self.n_clusters-1]))\n",
    "\n",
    "                self.w = pars[:self.n_clusters][np.newaxis, :]\n",
    "                self.mu = pars[self.n_clusters:self.n_clusters*2][np.newaxis, :]\n",
    "                self.nu = pars[self.n_clusters*2:self.n_clusters*3][np.newaxis, :]\n",
    "                self.sigma = pars[self.n_clusters*3:self.n_clusters*4][np.newaxis, :]\n",
    "\n",
    "                self.likelihood = -result.fun\n",
    "\n",
    "        self.aic = 2 * self.n_parameters - 2 * self.likelihood\n",
    "        self.bic = - 2 * self.likelihood + self.n_parameters * np.log(self.data.shape[0])\n",
    "\n",
    "    def fit_multiproc(self, n_tries=4, n_proc=4, disp=False):\n",
    "\n",
    "        pool = Pool(n_proc)\n",
    "\n",
    "        print 'starting pool'\n",
    "        results = pool.map(_fit, [(self.data, self.n_clusters)] * n_tries)\n",
    "        print 'ready'\n",
    "\n",
    "        print results\n",
    "\n",
    "        pool.close()\n",
    "\n",
    "        for result in results:\n",
    "            self.previous_likelihoods.append(-result.fun)\n",
    "            self.previous_pars.append(result.x)\n",
    "\n",
    "            if -result.fun > self.likelihood:\n",
    "\n",
    "                pars = result.x\n",
    "                pars = np.insert(pars, self.n_clusters-1, 1 - np.sum(pars[:self.n_clusters-1]))\n",
    "\n",
    "                self.w = pars[:self.n_clusters][np.newaxis, :]\n",
    "                self.mu = pars[self.n_clusters:self.n_clusters*2][np.newaxis, :]\n",
    "                self.nu = pars[self.n_clusters*2:self.n_clusters*3][np.newaxis, :]\n",
    "                self.sigma = pars[self.n_clusters*3:self.n_clusters*4][np.newaxis, :]\n",
    "\n",
    "                self.likelihood = -result.fun\n",
    "\n",
    "        self.aic = 2 * self.n_parameters - 2 * self.likelihood\n",
    "        self.bic = - 2 * self.likelihood + self.n_parameters * np.log(self.data.shape[0])\n",
    "\n",
    "    def plot_fit(self):\n",
    "        # Create indiviudal pds\n",
    "\n",
    "        t = np.linspace(0, self.data.max(), 100)\n",
    "        pdfs = self.w * exgauss_pdf(t[:, np.newaxis], self.mu, self.nu, self.sigma)\n",
    "\n",
    "        sns.distplot(self.data)\n",
    "        plt.plot(t, pdfs, c='k', alpha=0.5)\n",
    "\n",
    "        plt.plot(t, np.sum(pdfs, 1), c='k', lw=2)\n",
    "        \n",
    "class Scaler(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.min = X.min()\n",
    "        self.max = X.max()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X -= self.min\n",
    "        X /= self.max\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Starting the actual fitting:\n",
    " \n",
    "# Select the smoothing kernel\n",
    "fwhms = [0.15, 0.3, 0.6, 1.2, 2.4]\n",
    "# Set the number of mixtures\n",
    "ns = [1,2,3,4,5,6,7,8,9]\n",
    "# Subject ids\n",
    "subject_ids = [13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "# Stains\n",
    "stains = ['CALR', 'FER', 'GABRA3', 'GAD6567', 'MBP', 'PARV', 'SERT', 'SMI32', 'SYN', 'TH', 'TRANSF', 'VGLUT1']\n",
    "# data type\n",
    "data_types = ['intensity', 'gradient_2D']\n",
    "\n",
    "# PBS array is used on lisa so that you can submit alot of similiar jobs to the cue. We have 2016 jobs to submit \n",
    "#    (12 stains * 9 number of clusters * 7 tissue blocks * 5 fwhms * 2 data types)\n",
    "if 'PBS_ARRAYID' in os.environ.keys():\n",
    "    PBS_ARRAYID = int(os.environ['PBS_ARRAYID'])\n",
    "else:\n",
    "    PBS_ARRAYID = 0\n",
    "\n",
    "fwhm, n_clusters, stain, subject_id, data_type = list(itertools.product(fwhms, ns, stains, subject_ids, data_types))[PBS_ARRAYID]\n",
    "print n_clusters, fwhm, stain, subject_id, data_type\n",
    "\n",
    "results = []\n",
    "# The different training and test set partitions\n",
    "partitions = [{'train_name': 'All-Data-In-Mask'},\n",
    "               {'train_name': 'CV_set1_1', 'test_name': 'CV_set1_2'},\n",
    "               {'train_name': 'CV_set1_2', 'test_name': 'CV_set1_1'},\n",
    "               {'train_name': 'CV_set2_1', 'test_name': 'CV_set2_2'},\n",
    "               {'train_name': 'CV_set2_2', 'test_name': 'CV_set2_1'},\n",
    "               {'train_name': 'CV_set3_1', 'test_name': 'CV_set3_2'},\n",
    "               {'train_name': 'CV_set3_2', 'test_name': 'CV_set3_1'}]\n",
    "\n",
    "for partition in partitions:\n",
    "    print('Current train set: %s' % partition['train_name'])\n",
    "   \n",
    "    # Always fit model to train data\n",
    "    # Load data\n",
    "    train_set = pandas.read_pickle(os.path.join(os.environ['HOME'], \n",
    "                        'data/post_mortem/crossval_%s_feature_vectors/%s_%s_%s.pkl' %(data_type, subject_id, fwhm, partition['train_name'])))\n",
    "\n",
    "    # Select stain\n",
    "    train_set = train_set[stain]\n",
    "\n",
    "    # Remove nan-values\n",
    "    train_set = train_set[~pandas.isnull(train_set)]\n",
    "\n",
    "    # Select reasonable subsample to decrease computational burden.\n",
    "    # Get random evenly spaced (should be evenly spaced to make sure a representative sub sample\n",
    "    # of the spatial organisation of the STN is selected) sub sample of 15% to reduce size\n",
    "    # Getting exactly 15% is tricky because of varying original shapes (sample sizes), but this\n",
    "    # approach gets the value closest to and minimum of 15%\n",
    "    step_size = int(train_set.shape[0] / (train_set.shape[0]*.15))\n",
    "    train_set = train_set[::step_size]\n",
    "\n",
    "    # The train and test partitions are normalized per partition seperately. In a previous attempt we normalised \n",
    "    #   the test partition by the train partition. This however in the end led tot inf values for the the loglikelihood.\n",
    "    #   Since it could happen that the test partition had values that were outside the min/max of the train partition and \n",
    "    #   would therefore get a zero as likelihood -> log(0) == inf.\n",
    "    scaler = Scaler()\n",
    "    scaler.fit(train_set)\n",
    "    train_set = scaler.transform(train_set)\n",
    "\n",
    "    # Check if model is already saved to disk. If so, load model\n",
    "    path_name = os.path.join(os.environ['HOME'], 'data', 'post_mortem', 'ml_clusters_cross_validated_%s' %data_type)\n",
    "    if not os.path.exists(path_name):\n",
    "        os.makedirs(path_name)\n",
    "    pickle_fn = os.path.join(os.environ['HOME'], 'data', 'post_mortem', 'ml_clusters_cross_validated_%s' %data_type, '%s_%s_%s_%s_%s.pkl' %(subject_id, fwhm, stain, n_clusters, partition['train_name']))\n",
    "\n",
    "    if os.path.isfile(pickle_fn):\n",
    "        # Model was already trained & saved, so load\n",
    "        with open(pickle_fn, 'r') as f:\n",
    "            model = pkl.load(f)\n",
    "    else:\n",
    "        # Model does not yet exist, so train now\n",
    "        # Create & fit model on train set\n",
    "        model = SimpleExgaussMixture(train_set, n_clusters)\n",
    "        model.fit_multiproc(n_tries=n_proc, n_proc=n_proc)\n",
    "        pkl.dump(model, open(pickle_fn, 'w'))\n",
    "\n",
    "    # Append to results\n",
    "    results.append({'train': partition['train_name'], 'test': partition['train_name'],\n",
    "                   'll': model.get_likelihood_data(train_set),\n",
    "                   'aic': model.get_aic_data(train_set),\n",
    "                   'bic': model.get_bic_data(train_set)})\n",
    "\n",
    "\n",
    "    # If a test-set is provided, check cross-validation model fit\n",
    "    if 'test_name' in partition.keys():\n",
    "        # Load test data\n",
    "        test_set = pandas.read_pickle(os.path.join(os.environ['HOME'], 'data/post_mortem/crossval_%s_feature_vectors/%s_%s_%s.pkl' %(data_type, subject_id, fwhm, partition['test_name'])))\n",
    "        \n",
    "        # select stain\n",
    "        test_set = test_set[stain]    \n",
    "\n",
    "        # Remove nan/Nones\n",
    "        test_set = test_set[~pandas.isnull(test_set)]\n",
    "\n",
    "        # Subsample\n",
    "        step_size = int(test_set.shape[0] / (test_set.shape[0]*.15))\n",
    "        test_set = test_set[::step_size]\n",
    "\n",
    "        # Scale\n",
    "        scaler = Scaler()\n",
    "        scaler.fit(test_set)\n",
    "        test_set = scaler.transform(test_set)\n",
    "\n",
    "        # Append to results\n",
    "        results.append({'train': partition['train_name'], 'test': partition['test_name'],\n",
    "                       'll': model.get_likelihood_data(test_set),\n",
    "                       'aic': model.get_aic_data(test_set),\n",
    "                       'bic': model.get_bic_data(test_set)})\n",
    "\n",
    "results = pandas.DataFrame(results)\n",
    "results['subject_id'], results['stain'], results['fwhm'], results['n_clusters'] = subject_id, stain, fwhm, n_clusters\n",
    "# Saving the data to pandas\n",
    "results_fn = os.path.join(os.environ['HOME'], 'data', 'post_mortem', 'ml_clusters_cross_validated_%s' %(data_type), '%s_%s_%s_%s_all.pandas' %(subject_id, fwhm, stain, n_clusters))\n",
    "results.to_pickle(results_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Mixture analysis \n",
    "#### 4b1) After training and fitting the intensity and gradient vectors we want to plot the gradient vectors\n",
    "    As a sanity check to see whether the gradients are:\n",
    "        - not driven by borders\n",
    "        - large changes in gradients match what we see in the intensity figures\n",
    "        - the histograms look some what plausible\n",
    "     we will plot the data in a similar manner as what we did for the intensity pdfs as in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/13095/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/13095/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/13095/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/13095/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/13095/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/13095/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14037/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14037/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14037/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14037/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14037/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14037/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14051/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14051/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14051/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14051/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14051/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14051/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14069/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14069/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14069/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14069/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/14069/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/14069/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15033/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15033/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15033/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15033/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15033/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15033/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15035/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15035/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15035/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15035/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15035/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15035/intensity_gradient_2D_2.4.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15055/intensity_gradient_2D_0.15.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15055/intensity_gradient_2D_0.3.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15055/intensity_gradient_2D_0.6.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15055/intensity_gradient_2D_1.2.pdf already plotted\n",
      "/home/mkeuken1/data/post_mortem/new_data_format/15055/images.hdf5\n",
      "/home/mkeuken1/data/post_mortem/visualize_stains_v2/15055/intensity_gradient_2D_2.4.pdf already plotted\n"
     ]
    }
   ],
   "source": [
    "# Plotting the gradient vector data \n",
    "# Plotting the gradients in a similair manner as the intensity pdf\n",
    "\n",
    "# Import several functions and set a few style features:\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pystain import StainDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Ensure that the color coding is normalized between the min and max per stain\n",
    "#   We are using the same color range as what we used for the intensity plots in section 2):\n",
    "def cmap_hist(data, bins=None, cmap=plt.cm.hot, vmin=None, vmax=None):\n",
    "    n, bins, patches = plt.hist(data, bins=bins)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    \n",
    "    if vmin is None:\n",
    "        vmin = data.min()\n",
    "    if vmax is None:\n",
    "        vmax = data.max()\n",
    "\n",
    "    # scale values to interval [0,1]\n",
    "    col = (bin_centers - vmin) / vmax\n",
    "\n",
    "    for c, p in zip(col, patches):\n",
    "        plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "# The code to visualize the data:\n",
    "\n",
    "# Which tissue blocks are we going to visualize? \n",
    "subject_ids = [13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "\n",
    "# Create the figures per stain, per tissue block, for the all three FWHM kernel:\n",
    "for gradient_type in ['gradient_2D']: # could also plot 'gradient' here (ie 3D gradient) but we didnt fit these\n",
    "    for subject_id in subject_ids:\n",
    "        for fwhm in [0.15, 0.3, 0.6, 1.2, 2.4]:\n",
    "            dataset = StainDataset(subject_id, fwhm=fwhm)\n",
    "\n",
    "            d = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/%s/' % (subject_id)\n",
    "\n",
    "            if not os.path.exists(d):\n",
    "                os.makedirs(d) \n",
    "\n",
    "            fn = os.path.join(d, 'intensity_%s_%s.pdf' % (gradient_type, fwhm))\n",
    "                \n",
    "            if os.path.isfile(fn):\n",
    "                print('%s already plotted' %fn)\n",
    "                continue\n",
    "\n",
    "            pdf = PdfPages(fn)\n",
    "\n",
    "            for i, stain in enumerate(dataset.stains):\n",
    "                \n",
    "                print 'Plotting %s' % stain\n",
    "                plt.figure()\n",
    "                \n",
    "                # Get appropriate gradient type from h5File\n",
    "                if gradient_type == 'gradient':\n",
    "                    key = 'data_smoothed_%s_thr_%s_gradient_image' % (fwhm, 3)\n",
    "                elif gradient_type == 'gradient_2D':\n",
    "                    key = 'data_smoothed_%s_thr_%s_gradient_image_2D' % (fwhm, 3)\n",
    "                \n",
    "                # thresholded mask area is where at least 3 masks overlay\n",
    "                data = dataset.h5file[key].value[dataset.thresholded_mask, i]\n",
    "                data = data[~np.isnan(data)]\n",
    "                vmax = np.percentile(data, 99)\n",
    "                vmin = np.min(data)\n",
    "                bins = np.linspace(0, vmax, 100)\n",
    "                cmap_hist(data, bins, plt.cm.hot, vmin=vmin, vmax=vmax)\n",
    "                plt.title(stain)\n",
    "                plt.savefig(pdf, format='pdf')\n",
    "\n",
    "                plt.close(plt.gcf())\n",
    "\n",
    "                plt.figure()\n",
    "\n",
    "                if not os.path.exists(d):\n",
    "                    os.makedirs(d)\n",
    "\n",
    "                for i, orientation in enumerate(['coronal', 'axial', 'sagittal']):\n",
    "                    for j, q in enumerate([.25, .5, .75]):\n",
    "                        ax = plt.subplot(3, 3, i + j*3 + 1)\n",
    "                        slice = dataset.get_proportional_slice(q, orientation)\n",
    "                        dataset.plot_slice(slice=slice, stain=stain, gradient=True, orientation=orientation, cmap=plt.cm.hot)\n",
    "                        ax.set_anchor('NW')\n",
    "\n",
    "                plt.gcf().set_size_inches(20, 20)\n",
    "                plt.suptitle(stain)\n",
    "                plt.savefig(pdf, format='pdf')\n",
    "                plt.close(plt.gcf())\n",
    "\n",
    "            pdf.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Mixture analysis \n",
    "#### 4b1) Do the exGaussian mixtures actually fit the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032\n"
     ]
    }
   ],
   "source": [
    "# Import several functions and set a few style features:\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy as sp\n",
    "sns.set_context('poster')\n",
    "sns.set_style('ticks')\n",
    "\n",
    "\n",
    "# It is a bit ugly but the plot_fit function used to generate the figures is not part of the pystain package. So the function \n",
    "#  needs to be defined here:\n",
    "def exgauss_pdf(x, mu, sigma, nu):\n",
    "\n",
    "    nu = 1./nu\n",
    "    p1 = nu / 2. * np.exp((nu/2.)  * (2 * mu + nu * sigma**2. - 2. * x))\n",
    "    p2 = sp.special.erfc((mu + nu * sigma**2 - x)/ (np.sqrt(2.) * sigma))\n",
    "\n",
    "    return p1 * p2\n",
    "\n",
    "class SimpleExgaussMixture(object):\n",
    "\n",
    "    def __init__(self, data, n_clusters):\n",
    "\n",
    "        self.data = data\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_parameters = n_clusters * 4 - 1\n",
    "        self.likelihood = -np.inf\n",
    "\n",
    "        self.previous_likelihoods = []\n",
    "        self.previous_pars = []\n",
    "\n",
    "    def get_likelihood_data(self, data):\n",
    "        \n",
    "        return mixed_exgauss_likelihood(data, self.w, self.mu, self.sigma, self.nu)\n",
    "    \n",
    "    def get_bic_data(self, data):\n",
    "        likelihood = self.get_likelihood_data(data)\n",
    "        return - 2 * likelihood + self.n_parameters * np.log(data.shape[0])\n",
    "       \n",
    "    def get_aic_data(self, data):\n",
    "        likelihood = self.get_likelihood_data(data)\n",
    "        return 2 * self.n_parameters - 2  * likelihood\n",
    "    \n",
    "    def _fit(self, **kwargs):\n",
    "        return _fit((self.data, self.n_clusters), **kwargs)\n",
    "\n",
    "    def fit(self, n_tries=1, **kwargs):\n",
    "        for run in np.arange(n_tries):\n",
    "\n",
    "            result = self._fit(**kwargs)\n",
    "            self.previous_likelihoods.append(-result.fun)\n",
    "\n",
    "            if -result.fun > self.likelihood:\n",
    "\n",
    "                pars = result.x\n",
    "                pars = np.insert(pars, self.n_clusters-1, 1 - np.sum(pars[:self.n_clusters-1]))\n",
    "\n",
    "                self.w = pars[:self.n_clusters][np.newaxis, :]\n",
    "                self.mu = pars[self.n_clusters:self.n_clusters*2][np.newaxis, :]\n",
    "                self.nu = pars[self.n_clusters*2:self.n_clusters*3][np.newaxis, :]\n",
    "                self.sigma = pars[self.n_clusters*3:self.n_clusters*4][np.newaxis, :]\n",
    "\n",
    "                self.likelihood = -result.fun\n",
    "\n",
    "        self.aic = 2 * self.n_parameters - 2 * self.likelihood\n",
    "        self.bic = - 2 * self.likelihood + self.n_parameters * np.log(self.data.shape[0])\n",
    "\n",
    "    def fit_multiproc(self, n_tries=4, n_proc=4, disp=False):\n",
    "\n",
    "        pool = Pool(n_proc)\n",
    "\n",
    "        print 'starting pool'\n",
    "        results = pool.map(_fit, [(self.data, self.n_clusters)] * n_tries)\n",
    "        print 'ready'\n",
    "\n",
    "        print results\n",
    "\n",
    "        pool.close()\n",
    "\n",
    "        for result in results:\n",
    "            self.previous_likelihoods.append(-result.fun)\n",
    "            self.previous_pars.append(result.x)\n",
    "\n",
    "            if -result.fun > self.likelihood:\n",
    "\n",
    "                pars = result.x\n",
    "                pars = np.insert(pars, self.n_clusters-1, 1 - np.sum(pars[:self.n_clusters-1]))\n",
    "\n",
    "                self.w = pars[:self.n_clusters][np.newaxis, :]\n",
    "                self.mu = pars[self.n_clusters:self.n_clusters*2][np.newaxis, :]\n",
    "                self.nu = pars[self.n_clusters*2:self.n_clusters*3][np.newaxis, :]\n",
    "                self.sigma = pars[self.n_clusters*3:self.n_clusters*4][np.newaxis, :]\n",
    "\n",
    "                self.likelihood = -result.fun\n",
    "\n",
    "        self.aic = 2 * self.n_parameters - 2 * self.likelihood\n",
    "        self.bic = - 2 * self.likelihood + self.n_parameters * np.log(self.data.shape[0])\n",
    "        \n",
    "    def plot_fit(self):\n",
    "        # Create indiviudal pds\n",
    "\n",
    "        xlim_max = np.percentile(self.data, q=97.5)\n",
    "\n",
    "        data_plot = self.data[self.data<=xlim_max]\n",
    "        \n",
    "        t = np.linspace(0, data_plot.max(), 1000)\n",
    "        pdfs = self.w * exgauss_pdf(t[:, np.newaxis], self.mu, self.nu, self.sigma)\n",
    "\n",
    "        sns.distplot(data_plot, kde=False, norm_hist=True)\n",
    "        plt.plot(t, pdfs, c='k', alpha=0.5, lw=3, )\n",
    "        plt.plot(t, np.sum(pdfs, 1), c='k', lw=3, linestyle=':')\n",
    "        plt.xlabel('')#Stain intensity (a.u.)')\n",
    "        plt.ylabel('')#Density')\n",
    "        sns.despine()\n",
    "        plt.gca().set_xlim([0, xlim_max])\n",
    "\n",
    "# The actual plotting of the data:\n",
    "# Select the smoothing kernel\n",
    "fwhms = [0.3, 0.6, 1.2, 2.4]\n",
    "# Set the number of mixtures\n",
    "ns = [1,2,3,4,5,6]#,7,8,9]\n",
    "# Subject ids\n",
    "subject_ids = [13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "\n",
    "# Stains\n",
    "stains = ['CALR', 'FER', 'GABRA3', 'GAD6567', 'MBP', 'PARV', 'SERT', 'SMI32', 'SYN', 'TH', 'TRANSF', 'VGLUT1']\n",
    "\n",
    "# data type\n",
    "data_types = ['intensity','gradient_2D']\n",
    "\n",
    "print len(list(itertools.product(fwhms, ns, stains, subject_ids, data_types)))\n",
    "all_missing = []\n",
    "\n",
    "for fwhm in fwhms:\n",
    "    for subject_id in subject_ids:\n",
    "        for stain in stains:\n",
    "            for data_type in data_types:\n",
    "                \n",
    "                # load data of this stain,sub,n_clusters,data_type\n",
    "                fns = ['/home/mkeuken1/data/post_mortem/ml_clusters_cross_validated_%s/%s_%s_%s_' %(data_type, subject_id, fwhm, stain) + str(x) + '_All-Data-In-Mask.pkl' for x in ns]\n",
    "                \n",
    "                fns_not_exist = [fn for fn in fns if not os.path.exists(fn)]\n",
    "                all_missing.append(fns_not_exist)\n",
    "                \n",
    "                models = [pkl.load(open(fn)) if os.path.exists(fn) else 0 for fn in fns]\n",
    "                \n",
    "                f, ax = plt.subplots(2, 3, sharex=True, sharey=True)\n",
    "                \n",
    "                for model_n in range(1, np.max(ns)+1):\n",
    "                    plt.subplot(2, 3, model_n)\n",
    "                    \n",
    "                    if not models[model_n-1] == 0:\n",
    "                        models[model_n-1].plot_fit()\n",
    "                        if model_n > 1:\n",
    "                            plt.title('%d clusters' %model_n)\n",
    "                        else:\n",
    "                            plt.title('%d cluster' %model_n)\n",
    "                            \n",
    "#                plt.xlabel('staining %s' %stain)\n",
    "                subj_id_formatted = str(subject_id)[:2] + '-' + str(subject_id)[2:]\n",
    "                if data_type == 'gradient_2D':\n",
    "                    dtype_formatted = '%s ' %stain\n",
    "                    xlab = 'Gradient magnitude (a.u.)'\n",
    "                else:\n",
    "                    dtype_formatted = '%s' %stain\n",
    "                    xlab = 'Immunoreactivity (a.u.)'\n",
    "                    \n",
    "                plt.suptitle('%s #%s' %(dtype_formatted, subj_id_formatted))\n",
    "                plt.gcf().set_size_inches((20*2/3., 10*2/3.))\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                plt.gcf().text(0.5, 0.00, xlab, ha='center')\n",
    "                plt.gcf().text(0.00, 0.5, 'Density', va='center', rotation='vertical')\n",
    "                \n",
    "                \n",
    "                f = plt.gcf()\n",
    "                pdf = PdfPages('/home/mkeuken1/data/post_mortem/visualize_stains_v2/fits/%s_%s_%s_%s' %(subject_id, fwhm, stain, data_type))\n",
    "                f.savefig(pdf,  format='pdf', bbox_inches='tight')\n",
    "                pdf.close()\n",
    "                plt.close()\n",
    "#                break\n",
    "#            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4) Mixture analysis \n",
    "#### 4c) How many clusters fit the data best?\n",
    "\n",
    "Now that we plotted the gradient vectors and the histograms seem plausible we want to see what the winning model is in terms of number of clusters per stain.\n",
    "    We will do this in four different ways:\n",
    "        - bic\n",
    "        - aic\n",
    "        - cross validation loglikelihood within tissue block\n",
    "        \n",
    "#### 4c1) Getting the winning models based on the AIC and BIC values\n",
    "   What we want to do is to determine what the number of mixtures fits the data best per stain.\n",
    "   We have 6 different train and test partitions so it is a bit of a shame if we wouldnt do anything with them.\n",
    "       What we do in the following part seperately for AIC and BIC values is to\n",
    "           - calculate mean rank per number of clusters (mean over training datasets, note that there is only a single train dataset)\n",
    "           - plot them (ensure that the plot actually shows all the datapoints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean rank number for the different cluster nubmers based on the AIC and BIC \n",
    "# Once we know for a given subject what the prefered number of mixtures is per stain we can plot it. \n",
    "\n",
    "# Import several functions and set a few style features:\n",
    "import pandas\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# Select the smoothing kernel\n",
    "fwhms = [1.2]\n",
    "\n",
    "for fwhm in fwhms:\n",
    "    for data_type in ['intensity', 'gradient_2D']:\n",
    "        # Read all the panda data that have 1-7 clusters\n",
    "        fns = glob.glob('/home/mkeuken1/data/post_mortem/ml_clusters_cross_validated_%s/pandas_%s/*_all.pandas' %(data_type, str(fwhm)))\n",
    "        fns = [x for x in fns if int(x.split('_')[3]) < 8]\n",
    "        df = pandas.concat([pandas.read_pickle(fn) for fn in fns])\n",
    "\n",
    "        # Select all rows (idx) to use for BIC/AIC model comparison\n",
    "        idx_bicaic = df['train']=='All-Data-In-Mask'\n",
    "        dfBICAIC = df[idx_bicaic].copy()\n",
    "\n",
    "        # Calculate normalized aic and bic scores (for ranking)\n",
    "        dfBICAIC['aic_rank'] = np.nan \n",
    "        dfBICAIC['bic_rank'] = np.nan \n",
    "\n",
    "        # Loop over all combinations of subject, stain, training set, plus metric (aic / bic) to calculate ranks\n",
    "        tmp = dfBICAIC.drop('fwhm', axis=1).pivot_table(index=['subject_id','stain', 'train'], columns=['n_clusters'])\n",
    "        for metric in ['aic', 'bic']:\n",
    "            for subj in tmp[metric].reset_index()['subject_id'].unique():\n",
    "                for stain in tmp[metric].reset_index()['stain'].unique():\n",
    "                    for train in tmp[metric].reset_index()['train'].unique():\n",
    "                        tmp[metric + '_rank'].loc[subj].loc[stain].loc[train] = tmp[metric].loc[subj].loc[stain].loc[train].rank()\n",
    "\n",
    "\n",
    "        # Calculate mean rank per number of clusters based on AIC\n",
    "        # Reset index and groupby subj id and stain to calculate mean rank per number of clusters (mean over training datasets, note that for \n",
    "        #   AIC and BIC we only have a single training set: 15% of the total data in STN mask.)\n",
    "        tmp2 = tmp.reset_index().copy()\n",
    "        tmp2 = tmp2.groupby(['subject_id', 'stain']).mean()['aic_rank']\n",
    "        tmp2 = pandas.melt(tmp2.reset_index(), id_vars=['subject_id', 'stain'], value_vars=[1,2,3,4,5,6], value_name='mean_rank')\n",
    "\n",
    "        best_models_aic = tmp2.groupby(['subject_id', 'stain'])['mean_rank'].apply(lambda x: np.nanargmin(x)+1)\n",
    "        best_models_aic = best_models_aic.reset_index(name='winning_model')\n",
    "\n",
    "        # Calculate mean rank per number of clusters based on BIC\n",
    "        # Reset index and groupby subj id and stain to calculate mean rank per number of clusters (mean over training datasets, note that for \n",
    "        #   AIC and BIC we only have a single training set: 15% of the total data in STN mask.\n",
    "        tmp3 = tmp.reset_index().copy()\n",
    "        tmp3 = tmp3.groupby(['subject_id', 'stain']).mean()['bic_rank']\n",
    "        tmp3 = pandas.melt(tmp3.reset_index(), id_vars=['subject_id', 'stain'], value_vars=[1,2,3,4,5,6], value_name='mean_rank')\n",
    "\n",
    "        best_models_bic = tmp3.groupby(['subject_id', 'stain'])['mean_rank'].apply(lambda x: np.nanargmin(x)+1)\n",
    "        best_models_bic = best_models_bic.reset_index(name='winning_model')\n",
    "\n",
    "        # Creating extra dummy columns and concatenating both AIC and BIC in one df\n",
    "        best_models_aic['metric'] = 'aic'\n",
    "        best_models_bic['metric'] = 'bic'\n",
    "        best_models = pandas.concat([best_models_bic, best_models_aic])\n",
    "\n",
    "        # Lets now plot the winning cluster per stain per subject for both AIC and BIC seperately. \n",
    "        loopdict = [{'name':'aic', 'df': best_models_aic}, {'name': 'bic', 'df': best_models_bic}]\n",
    "        for metric in loopdict:\n",
    "            fac = sns.swarmplot('stain', 'winning_model', 'subject_id', metric['df'], split=True, size=10, alpha=1, )\n",
    "            sns.despine()\n",
    "            fac.set_ylim((.5, 6.5))\n",
    "            ylims = fac.get_ylim()\n",
    "            xs = np.arange(0.5, 11.5, 1)\n",
    "            fac.vlines(x=xs, ymin=0, ymax=ylims[1], linewidths=1, linestyle='--', alpha=.5)\n",
    "            fac.set_ylabel('Number of clusters')\n",
    "\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "            plt.gcf().set_size_inches(30, 6)\n",
    "            f = plt.gcf()\n",
    "            pdf = PdfPages('/home/mkeuken1/data/post_mortem/visualize_stains_v1/model_comparison_clusters/fwhm_%s/'%str(fwhm) + metric['name'] + '_' + data_type )\n",
    "            f.savefig(pdf,  format='pdf', bbox_inches='tight')\n",
    "            pdf.close()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c2a) Getting the winning models based on the withincross validation\n",
    "   What we want to do is to determine what the number of mixtures fits the data best per stain.\n",
    "   We have 6 different train and test partitions so it is a bit of a shame if we wouldnt do anything with them.\n",
    "       What we do in the following part seperately for within and cross validation values is to\n",
    "           - calculate mean rank per number of clusters (mean over training datasets)\n",
    "           - plot them (ensure that the plot actually shows all the datapoints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean rank number for the different cluster nubmers based on the cross-validation performance \n",
    "# Once we know for a given subject what the prefered number of mixtures is per stain we can plot it. \n",
    "\n",
    "# Import several functions and set a few style features:\n",
    "import pandas\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "# Select the smoothing kernel\n",
    "fwhms = [0.3, 0.6, 1.2, 2.4]\n",
    "max_cluster = 6\n",
    "consider_clusters = np.arange(1, max_cluster+1).tolist()\n",
    "\n",
    "for fwhm in fwhms:\n",
    "    for data_type in ['intensity', 'gradient_2D']:\n",
    "        # Read all the panda data\n",
    "        df = pandas.concat([pandas.read_pickle(fn) for fn in glob.glob('/home/mkeuken1/data/post_mortem/ml_clusters_cross_validated_%s/*_%s_*.pandas' %(data_type, str(fwhm)))])\n",
    "\n",
    "        # Select all rows (idx) to use for BIC/AIC model comparison\n",
    "        idx_cv = df['train']!=df['test']\n",
    "        dfcv = df[idx_cv]\n",
    "\n",
    "        # Calculate normalized cross-validated log likelihood scores (for ranking)\n",
    "        dfcv['ll_rank'] = np.nan \n",
    "\n",
    "        # Loop over all combinations of subject, stain, training set, plus metric (log likelihood) to calculate ranks\n",
    "        tmp = dfcv.drop('fwhm', axis=1).pivot_table(index=['subject_id','stain', 'train'], columns=['n_clusters'])\n",
    "        for metric in ['ll']:\n",
    "            for subj in tmp[metric].reset_index()['subject_id'].unique():\n",
    "                for stain in tmp[metric].reset_index()['stain'].unique():\n",
    "                    for train in tmp[metric].reset_index()['train'].unique():\n",
    "                        tmp[metric + '_rank'].loc[subj].loc[stain].loc[train] = tmp[metric].loc[subj].loc[stain].loc[train].rank()\n",
    "\n",
    "        # Calculate mean rank per number of clusters based on cross-validation\n",
    "        # Reset index and groupby subj id and stain to calculate mean rank per number of clusters (mean over training datasets, note that for \n",
    "        #   cross validation we have 3 different partitions and 2 directions)\n",
    "        tmp2 = tmp.reset_index().copy()\n",
    "        tmp2 = tmp2.groupby(['subject_id', 'stain']).mean()['ll_rank']\n",
    "        tmp2 = pandas.melt(tmp2.reset_index(), id_vars=['subject_id', 'stain'], value_vars=consider_clusters, value_name='mean_rank')\n",
    "\n",
    "        # Here, we get the winning models per subject/stain combination. Note that, contrary to the AIC/BIC, \n",
    "        # the winning model has the *largest* likelihood (instead of smallest).\n",
    "        # Therefore we are not using the nanargmin but the nanargmax\n",
    "        best_models_ll = tmp2.groupby(['subject_id', 'stain'])['mean_rank'].apply(lambda x: np.nanargmax(x)+1)\n",
    "        best_models_ll = best_models_ll.reset_index(name='winning_model')\n",
    "\n",
    "        # Creating extra dummy columns and concatenating both AIC and BIC in one df\n",
    "        best_models_ll['metric'] = 'll_cv'\n",
    "\n",
    "        # Lets now plot the winning cluster per stain per subject for LL. \n",
    "        loopdict = [{'name':'ll_cv', 'df': best_models_ll}]\n",
    "        for metric in loopdict:\n",
    "            fac = sns.swarmplot('stain', 'winning_model', 'subject_id', metric['df'], split=True, size=10, alpha=1, )\n",
    "            sns.despine()\n",
    "            fac.set_ylim((.5, max_cluster + .5))\n",
    "            ylims = fac.get_ylim()\n",
    "            xs = np.arange(0.5, 11.5, 1)\n",
    "            fac.vlines(x=xs, ymin=0, ymax=ylims[1], linewidths=1, linestyle='--', alpha=.5)\n",
    "            fac.set_ylabel('Number of clusters')\n",
    "\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.gcf().set_size_inches(30, 6)\n",
    "            f = plt.gcf()\n",
    "            pdf = PdfPages('/home/mkeuken1/data/post_mortem/visualize_stains_v2/model_comparison_clusters/fwhm_%s/'%str(fwhm) + metric['name'] + '_' + data_type )\n",
    "            f.savefig(pdf,  format='pdf', bbox_inches='tight')\n",
    "            pdf.close()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c2b) A sanity check is to see whether the rank orders correlate when switching train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkout correlations between rank order of different partitionings of the data for cross-validation.\n",
    "## i.e., check if the rank order obtained by training on CVset1_1 is similar to the rank order obtained by \n",
    "## training on CVset1_2.\n",
    "tmp3 = tmp.reset_index()\n",
    "cvset11_ranks = tmp3.loc[tmp3['train']=='CV_set1_1']['ll_rank'].values.ravel()\n",
    "cvset12_ranks = tmp3.loc[tmp3['train']=='CV_set1_2']['ll_rank'].values.ravel()\n",
    "print(np.corrcoef(cvset11_ranks, cvset12_ranks))\n",
    "cvset21_ranks = tmp3.loc[tmp3['train']=='CV_set2_1']['ll_rank'].values.ravel()\n",
    "cvset22_ranks = tmp3.loc[tmp3['train']=='CV_set2_2']['ll_rank'].values.ravel()\n",
    "print(np.corrcoef(cvset21_ranks, cvset22_ranks))\n",
    "cvset31_ranks = tmp3.loc[tmp3['train']=='CV_set3_1']['ll_rank'].values.ravel()\n",
    "cvset32_ranks = tmp3.loc[tmp3['train']=='CV_set3_2']['ll_rank'].values.ravel()\n",
    "print(np.corrcoef(cvset31_ranks, cvset32_ranks))\n",
    "\n",
    "# All >0.6, so seems at least reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) What is the proportion that a voxel belongs to a given cluster?\n",
    "\n",
    "Here we want to know what the proportion is that a voxel belongs to a clusters. What that means is that for the model with a single exGauss, all voxels will belong to that cluster. It becomes a bit more interesting when you have more exGauss distributions fit to the data. In case of a mixture model with 2 different clusters, is it then the case the for most voxels it is clear that they belong to a single cluster (with 95% probability)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading in the data\n",
    "fwhms = [0.3, 1.2]\n",
    "# Set the number of mixtures\n",
    "ns = [1,2,3,4,5,6]\n",
    "# Subject ids\n",
    "subject_ids = [13095, 14037, 14051, 14069, 15033, 15035, 15055]\n",
    "# Stains\n",
    "stains = ['CALR', 'FER', 'GABRA3', 'GAD6567', 'MBP', 'PARV', 'SERT', 'SMI32', 'SYN', 'TH', 'TRANSF', 'VGLUT1']\n",
    "# data type\n",
    "data_types = ['intensity', 'gradient_2D']\n",
    "print len(list(itertools.product(fwhms, ns, stains, subject_ids, data_types)))\n",
    "\n",
    "df = pandas.DataFrame(columns=['data_type', 'test', 'train', 'subject_id', 'stain', 'fwhm', 'n_clusters', 'model'])\n",
    "for subject_id in subject_ids:\n",
    "    for data_type in data_types:\n",
    "        for stain in stains:\n",
    "            for n_clusters in ns:\n",
    "                for fwhm in fwhms:\n",
    "                    for cv_set in ['All-Data-In-Mask']:\n",
    "                        fn = '/home/mkeuken1/data/post_mortem/ml_clusters_cross_validated_%s/%s_%s_%s_%s_%s.pkl' %(data_type, subject_id, fwhm, stain, n_clusters, cv_set)\n",
    "                        print(fn)\n",
    "                        with open(fn, 'r') as f:\n",
    "                            model = pkl.load(f)\n",
    "                        df_new_row = pandas.DataFrame({'subject_id': subject_id,\n",
    "                                                  'fwhm': fwhm,\n",
    "                                                  'train_set': cv_set,\n",
    "                                                  'stain': stain,\n",
    "                                                  'n_clusters': n_clusters,\n",
    "                                                  'data_type': data_type,\n",
    "                                                  'fit_object': model}, index=[0])\n",
    "                        df = pandas.concat([df, df_new_row])\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proportion_surely_one_cluster(row):\n",
    "    \n",
    "    fit_object = row.fit_object\n",
    "    pdfs = fit_object.w * exgauss_pdf(fit_object.data[:, np.newaxis], fit_object.mu, fit_object.nu, fit_object.sigma)\n",
    "    \n",
    "    return ((pdfs.max(1) / pdfs.sum(1)) > 0.95).mean()\n",
    "\n",
    "df['Proportion of strong cluster assignments'] = df.apply(get_proportion_surely_one_cluster, 1)\n",
    "df['Number of clusters'] = df['n_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was the previous plotting style\n",
    "# #sns.palplot(sns.color_palette('Set3', 12))\n",
    "# for fwhm in fwhms:\n",
    "#     for data_type in data_types:\n",
    "#         df_data_type = df.loc[(df['data_type']==data_type) & (df['fwhm']==fwhm)]\n",
    "#         tmp = df_data_type.groupby(['subject_id', 'Number of clusters', 'stain']).mean()\n",
    "#         tmp['dummy'] = 1\n",
    "#         fac = sns.factorplot('dummy', 'Proportion of strong cluster assignments', 'stain', tmp.reset_index(), col='Number of clusters', col_wrap=2, kind='bar', palette=sns.color_palette('Set3', 12), aspect=4)\n",
    "\n",
    "#         fac.set_ylabels('')\n",
    "#         fac.set_axis_labels(x_var='')\n",
    "#         fac.set_xlabels('')\n",
    "#         fac.set_xticklabels([])\n",
    "#         plt.savefig('/home/mkeuken1/data/post_mortem/visualize_stains_v1/voxel_assignment/voxel_assignments_%s_%s.pdf' %(data_type, str(fwhm)))\n",
    "\n",
    "\n",
    "# updated, better plot below\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "for fwhm in fwhms:\n",
    "    for data_type in data_types:\n",
    "        df_data_type = df.loc[(df['data_type']==data_type) & (df['fwhm']==fwhm) & (df['n_clusters']>1)]\n",
    "        tmp = df_data_type.groupby(['subject_id', 'Number of clusters', 'stain']).mean()\n",
    "        tmp['dummy'] = 1\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp['x'] = tmp['stain'].map(dict(zip(np.unique(tmp.stain), np.arange(len(np.unique(tmp.stain))))))\n",
    "        tmp['Number of clusters'] = tmp['Number of clusters'].astype(int)\n",
    "\n",
    "        f, ax = plt.subplots(2, 3, sharex=True, sharey=True)\n",
    "        \n",
    "        for i, n_cluster in enumerate(np.arange(2, 7)):\n",
    "            row_n = np.floor(i/3.)\n",
    "            col_n = i-row_n*3\n",
    "            print(row_n, col_n)\n",
    "            sns.barplot(x='dummy', y='Proportion of strong cluster assignments', hue='stain', \n",
    "                        data=tmp.loc[tmp['Number of clusters']==n_cluster], \n",
    "                        ax=ax[row_n, col_n], errwidth=2.5, capsize=0.01,\n",
    "                        linewidth=.5, edgecolor=\"k\")#, legend=False)\n",
    "            \n",
    "            ax[row_n, col_n].legend_.remove()\n",
    "            ax[row_n, col_n].set_ylabel('')\n",
    "            ax[row_n, col_n].set_xlabel('')\n",
    "            ax[row_n, col_n].set_xticklabels([])\n",
    "            ax[row_n, col_n].set_title('%d clusters' %n_cluster)\n",
    "        ax[-1, -1].axis('off')\n",
    "        ax[0, 2].legend(loc='center right', bbox_to_anchor=(1., -.60), ncol=2, \n",
    "                        columnspacing=.5, handletextpad=.5,\n",
    "                        fontsize='x-small')\n",
    "\n",
    "        suptxt = 'Gradient magnitude' if data_type=='gradient_2D' else 'Immunoreactivity'\n",
    "        plt.tight_layout()\n",
    "        plt.gcf().text(0.00, 0.5, 'Proportion strong cluster assignments', va='center', rotation='vertical')\n",
    "        plt.gcf().suptitle(suptxt)\n",
    "        plt.subplots_adjust(left=.09, top=.85)#, right=.95)\n",
    "\n",
    "        plt.gcf().set_size_inches(10, 6)\n",
    "        save_dir = '/home/mkeuken1/data/post_mortem/visualize_stains_v2/voxel_assignment'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        plt.savefig('/home/mkeuken1/data/post_mortem/visualize_stains_v2/voxel_assignment/voxel_assignments_%s_%s.pdf' %(data_type, str(fwhm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything below is tinkering around to see what's going on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
